"""
Meta-Pattern Analyzer - –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –º–µ—Ç–∞-–ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —á–µ—Ä–µ–∑ —Å–µ—Å—Å–∏–∏

üîÑ –¶–ï–õ–¨: –î–µ—Ç–µ–∫—Ü–∏—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–∞ –∫—Ä–æ—Å—Å-—Å–µ—Å—Å–∏–æ–Ω–Ω–æ–º —É—Ä–æ–≤–Ω–µ
üìä –ú–ï–¢–†–ò–ö–ò: Recurrence counting, pattern evolution tracking
üéØ –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ meta_patterns (Qdrant)
"""

import re
import logging
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from collections import defaultdict

logger = logging.getLogger(__name__)


@dataclass
class MetaPattern:
    """–ú–µ—Ç–∞-–ø–∞—Ç—Ç–µ—Ä–Ω (–ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è —á–µ—Ä–µ–∑ —Å–µ—Å—Å–∏–∏)"""
    pattern_id: str
    pattern_type: str  # theme, behavior, emotion, cognitive, relational
    description: str
    occurrences: int
    first_seen: str
    last_seen: str
    evidence: List[str] = field(default_factory=list)
    evolution: str = ""  # growing, stable, fading
    strength: float = 0.5  # 0.0 - 1.0


@dataclass
class PatternOccurrence:
    """–ï–¥–∏–Ω–∏—á–Ω–æ–µ –ø—Ä–æ—è–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
    pattern_id: str
    timestamp: str
    evidence: str
    context: str


class MetaPatternAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –º–µ—Ç–∞-–ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤:
    - THEME: –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ç–µ–º—ã (—Ä–∞–±–æ—Ç–∞, –æ—Ç–Ω–æ—à–µ–Ω–∏—è, –∑–¥–æ—Ä–æ–≤—å–µ)
    - BEHAVIOR: –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–∏–∑–±–µ–≥–∞–Ω–∏–µ, –æ—Ç–∫–ª–∞–¥—ã–≤–∞–Ω–∏–µ)
    - EMOTION: –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (—Ç—Ä–µ–≤–æ–≥–∞ –ø–µ—Ä–µ–¥ —Å–æ–±—ã—Ç–∏–µ–º)
    - COGNITIVE: –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏–∑–∞—Ü–∏—è)
    - RELATIONAL: –ü–∞—Ç—Ç–µ—Ä–Ω—ã –æ—Ç–Ω–æ—à–µ–Ω–∏–π (–≥—Ä–∞–Ω–∏—Ü—ã, –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å)
    """

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏
    PATTERN_DEFINITIONS = {
        # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        "work_overwhelm": {
            "type": "theme",
            "description": "–ü–µ—Ä–µ–≥—Ä—É–∂–µ–Ω–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç–æ–π",
            "indicators": [
                r"\b(?:–º–Ω–æ–≥–æ|–∫—É—á—É|–∑–∞–≤–∞–ª–µ–Ω[–∞—ã]?)\s+(?:—Ä–∞–±–æ—Ç—ã|–¥–µ–ª|–∑–∞–¥–∞—á)\b",
                r"\b–Ω–µ\s+—É—Å–ø–µ–≤–∞—é\b",
                r"\b–¥–µ–¥–ª–∞–π–Ω[–∞—ã]?\b",
                r"\b–≤—ã–≥–æ—Ä(?:–∞—é|–µ–ª[–∞–∏]?)\b"
            ]
        },
        "relationship_anxiety": {
            "type": "theme",
            "description": "–¢—Ä–µ–≤–æ–≥–∞ –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö",
            "indicators": [
                r"\b–±–æ—é—Å—å\s+–ø–æ—Ç–µ—Ä—è—Ç—å\b",
                r"\b–Ω–µ\s+(?:—É–≤–µ—Ä–µ–Ω[–∞—ã]?|–∑–Ω–∞—é)\s+–≤\s+(?:–Ω—ë–º|–Ω–µ–π|–Ω–∞—Å)\b",
                r"\b—Ä–µ–≤–Ω—É(?:—é|–µ—Ç|–µ–º)?\b",
                r"\b–∑–∞–≤–∏—Å–∏–º(?:–∞|—ã|–æ—Å—Ç—å)?\b"
            ]
        },
        "health_concern": {
            "type": "theme",
            "description": "–ë–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ –æ –∑–¥–æ—Ä–æ–≤—å–µ",
            "indicators": [
                r"\b–±–æ–ª–∏—Ç\b",
                r"\b—É—Å—Ç–∞–ª[–∞–∏]?\b",
                r"\b—Å–æ–Ω\b.*\b(?:–ø–ª–æ—Ö–æ–π|–Ω–µ\s+–≤—ã—Å—ã–ø–∞—é—Å—å)\b",
                r"\b—ç–Ω–µ—Ä–≥–∏–∏\s+(?:–Ω–µ—Ç|–º–∞–ª–æ)\b"
            ]
        },

        # –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        "avoidance_pattern": {
            "type": "behavior",
            "description": "–ü–∞—Ç—Ç–µ—Ä–Ω –∏–∑–±–µ–≥–∞–Ω–∏—è",
            "indicators": [
                r"\b–æ—Ç–∫–ª–∞–¥—ã(?:–≤–∞—é|–≤–∞–ª[–∞–∏]?)\b",
                r"\b–∏–∑–±–µ–≥–∞(?:—é|–ª[–∞–∏]?)\b",
                r"\b–Ω–µ\s+—Ö–æ—á—É\s+(?:–¥—É–º–∞—Ç—å|–≥–æ–≤–æ—Ä–∏—Ç—å)\b",
                r"\b–ª—É—á—à–µ\s+–ø–æ—Ç–æ–º\b"
            ]
        },
        "perfectionism": {
            "type": "behavior",
            "description": "–ü–µ—Ä—Ñ–µ–∫—Ü–∏–æ–Ω–∏–∑–º",
            "indicators": [
                r"\b(?:–¥–æ–ª–∂–Ω[–æ–∞]?\s+–±—ã—Ç—å\s+)?–∏–¥–µ–∞–ª—å–Ω[–æ–∞]?\b",
                r"\b–Ω–µ\s+–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ\s+—Ö–æ—Ä–æ—à[–æ–∞]?\b",
                r"\b–æ—à–∏–±–∫–∞.*–Ω–µ–¥–æ–ø—É—Å—Ç–∏–º[–∞—ã]?\b",
                r"\b–≤—Å—ë\s+–∏–ª–∏\s+–Ω–∏—á–µ–≥–æ\b"
            ]
        },
        "people_pleasing": {
            "type": "behavior",
            "description": "–£–≥–æ–¥–Ω–∏—á–µ—Å—Ç–≤–æ",
            "indicators": [
                r"\b–Ω–µ\s+–º–æ–≥—É\s+–æ—Ç–∫–∞–∑–∞—Ç—å\b",
                r"\b—á—Ç–æ\s+(?:–æ–±–æ\s+–º–Ω–µ\s+)?–ø–æ–¥—É–º–∞—é—Ç\b",
                r"\b–Ω–µ\s+—Ö–æ—á—É\s+(?:—Ä–∞—Å—Å—Ç—Ä–æ–∏—Ç—å|–æ–±–∏–¥–µ—Ç—å)\b",
                r"\b–≤—Å–µ–º\s+(?:—É–≥–æ–¥–∏—Ç—å|–Ω—Ä–∞–≤–∏—Ç—å—Å—è)\b"
            ]
        },

        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        "anticipatory_anxiety": {
            "type": "emotion",
            "description": "–¢—Ä–µ–≤–æ–≥–∞ –æ–∂–∏–¥–∞–Ω–∏—è",
            "indicators": [
                r"\b–ø–µ—Ä–µ–∂–∏–≤–∞—é\s+(?:–∑–∞—Ä–∞–Ω–µ–µ|–æ\s+–±—É–¥—É—â–µ–º)\b",
                r"\b–∞\s+–≤–¥—Ä—É–≥\b",
                r"\b—á—Ç–æ\s+–µ—Å–ª–∏\b.*\b–ø–ª–æ—Ö–æ[–µ–º—É]?\b",
                r"\b–±–æ—é—Å—å\s+(?:—á—Ç–æ\s+)?–±—É–¥–µ—Ç\b"
            ]
        },
        "guilt_cycle": {
            "type": "emotion",
            "description": "–¶–∏–∫–ª –≤–∏–Ω—ã",
            "indicators": [
                r"\b–≤–∏–Ω–æ–≤(?:–∞—Ç[–∞—ã]?|–∞)\b",
                r"\b–∫–∞–∫\s+(?:—è\s+)?–º–æ–≥(?:–ª–∞|—É)?\b",
                r"\b–Ω–µ\s+–¥–æ–ª–∂–Ω[–∞–æ—ã]?\s+–±—ã–ª[–∞–∏]?\b",
                r"\b–º–æ—è\s+–≤–∏–Ω–∞\b"
            ]
        },
        "emotional_suppression": {
            "type": "emotion",
            "description": "–ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ —ç–º–æ—Ü–∏–π",
            "indicators": [
                r"\b–Ω–µ–ª—å–∑—è\s+(?:–ø–ª–∞–∫–∞—Ç—å|–∑–ª–∏—Ç—å—Å—è)\b",
                r"\b–¥–µ—Ä–∂—É\s+–≤\s+—Å–µ–±–µ\b",
                r"\b–Ω–µ\s+–ø–æ–∫–∞–∑—ã–≤–∞—é\b",
                r"\b—Å–¥–µ—Ä–∂–∏–≤–∞(?:—é—Å—å|—Ç—å)\b"
            ]
        },

        # –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        "catastrophizing": {
            "type": "cognitive",
            "description": "–ö–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏–∑–∞—Ü–∏—è",
            "indicators": [
                r"\b–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ[–∞—ã]?\b",
                r"\b–∫–æ–Ω–µ—Ü\s+—Å–≤–µ—Ç–∞\b",
                r"\b–≤—Å—ë\s+–ø—Ä–æ–ø–∞–ª–æ\b",
                r"\b–Ω–∏–∫–æ–≥–¥–∞\s+–Ω–µ\s+(?:–±—É–¥–µ—Ç|–ø–æ–ª—É—á–∏—Ç—Å—è)\b"
            ]
        },
        "mind_reading": {
            "type": "cognitive",
            "description": "–ß—Ç–µ–Ω–∏–µ –º—ã—Å–ª–µ–π",
            "indicators": [
                r"\b–æ–Ω[–∞–∏]?\s+–¥—É–º–∞—é—Ç?\s+—á—Ç–æ\b",
                r"\b—É–≤–µ—Ä–µ–Ω[–∞—ã]?\s+—á—Ç–æ\s+–æ–Ω[–∞–∏]?\b",
                r"\b—Ç–æ—á–Ω–æ\s+(?:—Å—á–∏—Ç–∞–µ—Ç|–¥—É–º–∞–µ—Ç)\b",
                r"\b—è\s+–∑–Ω–∞—é\s+—á—Ç–æ\s+–æ–Ω[–∞–∏]?\b"
            ]
        },
        "black_white_thinking": {
            "type": "cognitive",
            "description": "–ß–µ—Ä–Ω–æ-–±–µ–ª–æ–µ –º—ã—à–ª–µ–Ω–∏–µ",
            "indicators": [
                r"\b–∏–ª–∏\s+–≤—Å—ë\s+–∏–ª–∏\s+–Ω–∏—á–µ–≥–æ\b",
                r"\b—Ç–æ–ª—å–∫–æ\s+(?:—Ö–æ—Ä–æ—à–æ|–ø–ª–æ—Ö–æ)\b",
                r"\b–Ω–∏–∫–æ–≥–¥–∞\b.*\b–≤—Å–µ–≥–¥–∞\b",
                r"\b–ø–æ–ª–Ω—ã–π\s+(?:–ø—Ä–æ–≤–∞–ª|—É—Å–ø–µ—Ö)\b"
            ]
        },

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –æ—Ç–Ω–æ—à–µ–Ω–∏–π
        "boundary_issues": {
            "type": "relational",
            "description": "–ü—Ä–æ–±–ª–µ–º—ã —Å –≥—Ä–∞–Ω–∏—Ü–∞–º–∏",
            "indicators": [
                r"\b–Ω–µ\s+(?:–º–æ–≥—É|—É–º–µ—é)\s+(?:–æ—Ç–∫–∞–∑–∞—Ç—å|—Å–∫–∞–∑–∞—Ç—å\s+–Ω–µ—Ç)\b",
                r"\b–Ω–∞—Ä—É—à–∞—é—Ç\s+(?:–º–æ–∏\s+)?–≥—Ä–∞–Ω–∏—Ü—ã\b",
                r"\b—Å–ª–∏—à–∫–æ–º\s+(?:–º–Ω–æ–≥–æ\s+)?–ø–æ–∑–≤–æ–ª—è—é\b",
                r"\b–Ω–µ\s+(?:–∑–∞—â–∏—â–∞—é|–æ—Ç—Å—Ç–∞–∏–≤–∞—é)\s+—Å–µ–±—è\b"
            ]
        },
        "codependency": {
            "type": "relational",
            "description": "–°–æ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å",
            "indicators": [
                r"\b–Ω–µ\s+–º–æ–≥—É\s+–±–µ–∑\s+(?:–Ω–µ–≥–æ|–Ω–µ—ë)\b",
                r"\b–º–æ—è\s+–∂–∏–∑–Ω—å\s+–∑–∞–≤–∏—Å–∏—Ç\b",
                r"\b—Å–ø–∞—Å–∞—é\b",
                r"\b–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω(?:–Ω–∞|–µ–Ω)\s+–∑–∞\s+(?:–µ–≥–æ|–µ—ë)\s+(?:—á—É–≤—Å—Ç–≤–∞|—Å—á–∞—Å—Ç—å–µ)\b"
            ]
        },
        "trust_issues": {
            "type": "relational",
            "description": "–ü—Ä–æ–±–ª–µ–º—ã —Å –¥–æ–≤–µ—Ä–∏–µ–º",
            "indicators": [
                r"\b–Ω–µ\s+(?:–º–æ–≥—É|—Ö–æ—á—É)\s+–¥–æ–≤–µ—Ä—è—Ç—å\b",
                r"\b–æ–±–º–∞–Ω(?:—É—Ç|–µ—Ç|—ã–≤–∞–ª–∏)\b",
                r"\b–ø—Ä–µ–¥–∞–¥(?:—É—Ç|—É—Ç)\b",
                r"\b–Ω–∏–∫–æ–º—É\s+–Ω–µ–ª—å–∑—è\s+–≤–µ—Ä–∏—Ç—å\b"
            ]
        }
    }

    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –º–µ—Ç–∞-–ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        # –ò—Å—Ç–æ—Ä–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
        self.user_patterns: Dict[int, Dict[str, MetaPattern]] = {}
        # –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ—è–≤–ª–µ–Ω–∏–π
        self.pattern_history: Dict[int, List[PatternOccurrence]] = {}

        logger.info("‚úÖ MetaPatternAnalyzer initialized")

    def analyze(
        self,
        user_id: int,
        text: str,
        context: Optional[Dict[str, Any]] = None
    ) -> List[MetaPattern]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –º–µ—Ç–∞-–ø–∞—Ç—Ç–µ—Ä–Ω—ã

        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç

        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö/–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–µ—Ç–∞-–ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        """
        text_lower = text.lower()
        detected_patterns = []
        current_time = datetime.now().isoformat()

        for pattern_id, pattern_def in self.PATTERN_DEFINITIONS.items():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            matches = []
            for indicator in pattern_def["indicators"]:
                match = re.search(indicator, text_lower, re.IGNORECASE)
                if match:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    start = max(0, match.start() - 30)
                    end = min(len(text), match.end() + 30)
                    evidence = text[start:end].strip()
                    matches.append(evidence)

            if matches:
                # –û–±–Ω–æ–≤–ª—è–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –ø–∞—Ç—Ç–µ—Ä–Ω
                pattern = self._update_pattern(
                    user_id=user_id,
                    pattern_id=pattern_id,
                    pattern_type=pattern_def["type"],
                    description=pattern_def["description"],
                    evidence=matches[0],  # –ü–µ—Ä–≤—ã–π –º–∞—Ç—á –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ
                    timestamp=current_time
                )
                detected_patterns.append(pattern)

        if detected_patterns:
            logger.info(f"üîÑ Detected {len(detected_patterns)} meta-patterns for user {user_id}: {[p.pattern_id for p in detected_patterns]}")

        return detected_patterns

    def _update_pattern(
        self,
        user_id: int,
        pattern_id: str,
        pattern_type: str,
        description: str,
        evidence: str,
        timestamp: str
    ) -> MetaPattern:
        """–û–±–Ω–æ–≤–∏—Ç—å –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å –º–µ—Ç–∞-–ø–∞—Ç—Ç–µ—Ä–Ω"""

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if user_id not in self.user_patterns:
            self.user_patterns[user_id] = {}

        if pattern_id in self.user_patterns[user_id]:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω
            pattern = self.user_patterns[user_id][pattern_id]
            pattern.occurrences += 1
            pattern.last_seen = timestamp
            if evidence not in pattern.evidence:
                pattern.evidence.append(evidence)
                # –•—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤
                pattern.evidence = pattern.evidence[-5:]

            # –û–±–Ω–æ–≤–ª—è–µ–º strength –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç—ã
            pattern.strength = min(1.0, 0.3 + (pattern.occurrences / 10) * 0.7)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —ç–≤–æ–ª—é—Ü–∏—é (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
            if pattern.occurrences > 3:
                pattern.evolution = "growing"
            elif pattern.occurrences == 1:
                pattern.evolution = "emerging"
            else:
                pattern.evolution = "stable"
        else:
            # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
            pattern = MetaPattern(
                pattern_id=pattern_id,
                pattern_type=pattern_type,
                description=description,
                occurrences=1,
                first_seen=timestamp,
                last_seen=timestamp,
                evidence=[evidence],
                evolution="emerging",
                strength=0.3
            )
            self.user_patterns[user_id][pattern_id] = pattern

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ—è–≤–ª–µ–Ω–∏–π
        occurrence = PatternOccurrence(
            pattern_id=pattern_id,
            timestamp=timestamp,
            evidence=evidence,
            context=""
        )
        if user_id not in self.pattern_history:
            self.pattern_history[user_id] = []
        self.pattern_history[user_id].append(occurrence)

        return pattern

    def get_user_patterns(self, user_id: int) -> List[MetaPattern]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –º–µ—Ç–∞-–ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        if user_id not in self.user_patterns:
            return []
        return list(self.user_patterns[user_id].values())

    def get_top_patterns(
        self,
        user_id: int,
        top_n: int = 3,
        by: str = "occurrences"
    ) -> List[MetaPattern]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ø –º–µ—Ç–∞-–ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            top_n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            by: –ö—Ä–∏—Ç–µ—Ä–∏–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ (occurrences, strength)

        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–æ–ø –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        """
        if user_id not in self.user_patterns:
            return []

        patterns = list(self.user_patterns[user_id].values())

        if by == "strength":
            patterns.sort(key=lambda x: x.strength, reverse=True)
        else:
            patterns.sort(key=lambda x: x.occurrences, reverse=True)

        return patterns[:top_n]

    def get_patterns_by_type(self, user_id: int, pattern_type: str) -> List[MetaPattern]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞"""
        if user_id not in self.user_patterns:
            return []

        return [
            p for p in self.user_patterns[user_id].values()
            if p.pattern_type == pattern_type
        ]

    def get_pattern_summary(self, user_id: int) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–≤–æ–¥–∫—É –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

        Returns:
            Dict —Å –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        """
        if user_id not in self.user_patterns:
            return {
                "total_patterns": 0,
                "by_type": {},
                "strongest_pattern": None,
                "most_frequent_pattern": None
            }

        patterns = list(self.user_patterns[user_id].values())

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º
        by_type: Dict[str, int] = defaultdict(int)
        for p in patterns:
            by_type[p.pattern_type] += 1

        # –ü–æ–∏—Å–∫ —Å–∏–ª—å–Ω–µ–π—à–µ–≥–æ –∏ —á–∞—Å—Ç–µ–π—à–µ–≥–æ
        strongest = max(patterns, key=lambda x: x.strength) if patterns else None
        most_frequent = max(patterns, key=lambda x: x.occurrences) if patterns else None

        return {
            "total_patterns": len(patterns),
            "by_type": dict(by_type),
            "strongest_pattern": strongest.pattern_id if strongest else None,
            "strongest_description": strongest.description if strongest else None,
            "most_frequent_pattern": most_frequent.pattern_id if most_frequent else None,
            "most_frequent_occurrences": most_frequent.occurrences if most_frequent else 0
        }

    def get_therapeutic_insight(self, user_id: int) -> Optional[str]:
        """
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–µ—Å–∫–∏–π –∏–Ω—Å–∞–π—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤

        Returns:
            –¢–µ–∫—Å—Ç –∏–Ω—Å–∞–π—Ç–∞ –∏–ª–∏ None
        """
        patterns = self.get_top_patterns(user_id, top_n=2, by="strength")

        if not patterns:
            return None

        if len(patterns) == 1:
            p = patterns[0]
            return f"–ó–∞–º–µ—á–∞—é –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω: {p.description}. –≠—Ç–æ –ø–æ—è–≤–ª—è–ª–æ—Å—å —É–∂–µ {p.occurrences} —Ä–∞–∑(–∞). –•–æ—Ç–∏—Ç–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å —ç—Ç–æ –ø–æ–¥—Ä–æ–±–Ω–µ–µ?"
        else:
            p1, p2 = patterns[0], patterns[1]
            return f"–í–∏–∂—É —Å–≤—è–∑—å –º–µ–∂–¥—É –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ '{p1.description}' –∏ '{p2.description}'. –í–æ–∑–º–æ–∂–Ω–æ, –æ–Ω–∏ –≤–ª–∏—è—é—Ç –¥—Ä—É–≥ –Ω–∞ –¥—Ä—É–≥–∞. –ß—Ç–æ –≤—ã –¥—É–º–∞–µ—Ç–µ –æ–± —ç—Ç–æ–º?"

    def format_for_storage(self, patterns: List[MetaPattern]) -> List[Dict[str, Any]]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è"""
        return [
            {
                "pattern_id": p.pattern_id,
                "pattern_type": p.pattern_type,
                "description": p.description,
                "occurrences": p.occurrences,
                "strength": p.strength,
                "evolution": p.evolution,
                "first_seen": p.first_seen,
                "last_seen": p.last_seen,
                "evidence": p.evidence[-3:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞
            }
            for p in patterns
        ]


# Singleton instance
_meta_analyzer = None


def get_meta_analyzer() -> MetaPatternAnalyzer:
    """–ü–æ–ª—É—á–∏—Ç—å singleton —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –º–µ—Ç–∞-–ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
    global _meta_analyzer
    if _meta_analyzer is None:
        _meta_analyzer = MetaPatternAnalyzer()
    return _meta_analyzer


# === –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ===
if __name__ == "__main__":
    analyzer = MetaPatternAnalyzer()

    user_id = 999

    # –¢–µ—Å—Ç: –ù–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    print("üîÑ META-PATTERN ANALYZER TEST\n" + "=" * 50)

    test_messages = [
        # –°–æ–æ–±—â–µ–Ω–∏–µ 1: –†–∞–±–æ—Ç–∞ + –∏–∑–±–µ–≥–∞–Ω–∏–µ
        "–û–ø—è—Ç—å –∑–∞–≤–∞–ª–µ–Ω–∞ —Ä–∞–±–æ—Ç–æ–π, —Å—Ç–æ–ª—å–∫–æ –¥–µ–ª –Ω–∞–∫–æ–ø–∏–ª–æ—Å—å. –û—Ç–∫–ª–∞–¥—ã–≤–∞—é –≤–∞–∂–Ω—ã–π —Ä–∞–∑–≥–æ–≤–æ—Ä —Å –Ω–∞—á–∞–ª—å–Ω–∏–∫–æ–º.",

        # –°–æ–æ–±—â–µ–Ω–∏–µ 2: –†–∞–±–æ—Ç–∞ + —Ç—Ä–µ–≤–æ–≥–∞ –æ–∂–∏–¥–∞–Ω–∏—è
        "–ù–µ —É—Å–ø–µ–≤–∞—é –∫ –¥–µ–¥–ª–∞–π–Ω—É. –ê –≤–¥—Ä—É–≥ –º–µ–Ω—è —É–≤–æ–ª—è—Ç? –ü–µ—Ä–µ–∂–∏–≤–∞—é –∑–∞—Ä–∞–Ω–µ–µ –æ –≤—Å—Ç—Ä–µ—á–µ –∑–∞–≤—Ç—Ä–∞.",

        # –°–æ–æ–±—â–µ–Ω–∏–µ 3: –û—Ç–Ω–æ—à–µ–Ω–∏—è + –≥—Ä–∞–Ω–∏—Ü—ã
        "–û–Ω –æ–ø—è—Ç—å –ø—Ä–æ—Å–∏—Ç –ø–æ–º–æ—â—å, –Ω–µ –º–æ–≥—É –æ—Ç–∫–∞–∑–∞—Ç—å. –ë–æ—é—Å—å –ø–æ—Ç–µ—Ä—è—Ç—å –µ–≥–æ –µ—Å–ª–∏ —Å–∫–∞–∂—É –Ω–µ—Ç.",

        # –°–æ–æ–±—â–µ–Ω–∏–µ 4: –†–∞–±–æ—Ç–∞ + –ø–µ—Ä—Ñ–µ–∫—Ü–∏–æ–Ω–∏–∑–º
        "–ü—Ä–æ–µ–∫—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∏–¥–µ–∞–ª—å–Ω—ã–º. –û—à–∏–±–∫–∞ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º–∞. –°–Ω–æ–≤–∞ –º–Ω–æ–≥–æ —Ä–∞–±–æ—Ç—ã –Ω–∞ –Ω–µ–¥–µ–ª–µ.",

        # –°–æ–æ–±—â–µ–Ω–∏–µ 5: –í–∏–Ω–∞ + –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ
        "–ß—É–≤—Å—Ç–≤—É—é —Å–µ–±—è –≤–∏–Ω–æ–≤–∞—Ç–æ–π —á—Ç–æ –æ—Ç–∫–∞–∑–∞–ª–∞. –î–µ—Ä–∂—É –≤ —Å–µ–±–µ –∑–ª–æ—Å—Ç—å, –Ω–µ–ª—å–∑—è –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å."
    ]

    for i, message in enumerate(test_messages):
        print(f"\nüìù Message {i+1}: {message[:50]}...")

        patterns = analyzer.analyze(user_id, message)

        for p in patterns:
            status = "üÜï" if p.occurrences == 1 else f"üîÑ x{p.occurrences}"
            print(f"   {status} {p.pattern_id}: {p.description} (strength: {p.strength:.2f})")

    # –°–≤–æ–¥–∫–∞
    print("\n" + "=" * 50)
    summary = analyzer.get_pattern_summary(user_id)
    print(f"üìä Summary:")
    print(f"   Total patterns: {summary['total_patterns']}")
    print(f"   By type: {summary['by_type']}")
    print(f"   Strongest: {summary['strongest_pattern']} - {summary['strongest_description']}")
    print(f"   Most frequent: {summary['most_frequent_pattern']} ({summary['most_frequent_occurrences']} times)")

    # –¢–æ–ø –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    top = analyzer.get_top_patterns(user_id, top_n=3)
    print(f"\nüéØ Top 3 patterns:")
    for p in top:
        bar = "‚ñà" * int(p.strength * 10) + "‚ñë" * (10 - int(p.strength * 10))
        print(f"   {p.description}: {bar} {p.strength:.0%} ({p.occurrences}x)")

    # –¢–µ—Ä–∞–ø–µ–≤—Ç–∏—á–µ—Å–∫–∏–π –∏–Ω—Å–∞–π—Ç
    insight = analyzer.get_therapeutic_insight(user_id)
    if insight:
        print(f"\nüí° Therapeutic insight:")
        print(f"   {insight}")

    print("\n‚úÖ Test completed!")
