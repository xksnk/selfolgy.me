"""
Answer Analyzer - –ì–ª–∞–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–∏—Å—Ç–µ–º—ã

üß† –ü–†–ò–ù–¶–ò–ü: "–ö–∞–∂–¥—ã–π –æ—Ç–≤–µ—Ç - —ç—Ç–æ —à—Ç—Ä–∏—Ö –≤ –ø–æ—Ä—Ç—Ä–µ—Ç–µ –¥—É—à–∏"
üî¨ –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: –ö–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
‚ö° –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨: –î–≤—É—Ö—Ñ–∞–∑–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (instant + deep)
üõ°Ô∏è –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–¨: –î–µ—Ç–µ–∫—Ü–∏—è –∫—Ä–∏–∑–∏—Å–æ–≤ –∏ –¥–µ–ª–∏–∫–∞—Ç–Ω–∞—è —Ä–∞–±–æ—Ç–∞
"""

import logging
import json
import asyncio
import os
from typing import Dict, Any, Optional, List
from datetime import datetime

from .analysis_config import AnalysisConfig
from .analysis_templates import AnalysisTemplates
from .ai_model_router import AIModelRouter
from .trait_extractor import TraitExtractor

# AI clients
try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    from anthropic import AsyncAnthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

logger = logging.getLogger(__name__)

class AnswerAnalyzer:
    """
    –ì–ª–∞–≤–Ω—ã–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç:
    - –£–º–Ω—ã–π –≤—ã–±–æ—Ä AI –º–æ–¥–µ–ª–∏ (AIModelRouter)
    - –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã (AnalysisTemplates)  
    - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–µ—Ä—Ç –ª–∏—á–Ω–æ—Å—Ç–∏ (TraitExtractor)
    - –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (AnalysisConfig)
    
    –°–æ–∑–¥–∞–µ—Ç –∂–∏–≤–æ–π, —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∏–π –ø–æ—Ä—Ç—Ä–µ—Ç –ª–∏—á–Ω–æ—Å—Ç–∏.
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–∞–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.config = AnalysisConfig()
        self.templates = AnalysisTemplates()
        self.ai_router = AIModelRouter()
        self.trait_extractor = TraitExtractor()

        # AI –∫–ª–∏–µ–Ω—Ç—ã
        self.openai_client = None
        self.anthropic_client = None

        if OPENAI_AVAILABLE:
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key:
                self.openai_client = AsyncOpenAI(api_key=api_key)
                logger.info("‚úÖ OpenAI client initialized for AnswerAnalyzer")
            else:
                logger.warning("‚ö†Ô∏è OPENAI_API_KEY not found")

        if ANTHROPIC_AVAILABLE:
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if api_key:
                self.anthropic_client = AsyncAnthropic(api_key=api_key)
                logger.info("‚úÖ Anthropic client initialized for AnswerAnalyzer")
            else:
                logger.warning("‚ö†Ô∏è ANTHROPIC_API_KEY not found")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–±–æ—Ç—ã
        self.analysis_stats = {
            "total_analyses": 0,
            "successful_analyses": 0,
            "crisis_detections": 0,
            "breakthrough_moments": 0,
            "avg_processing_time_ms": 0
        }

        logger.info("üî¨ AnswerAnalyzer initialized - ready to analyze souls")
    
    async def analyze_answer(
        self,
        question_data: Dict[str, Any], 
        user_answer: str,
        user_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        –ì–õ–ê–í–ù–´–ô –ú–ï–¢–û–î - –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Args:
            question_data: –ü–æ–ª–Ω—ã–π –æ–±—ä–µ–∫—Ç –≤–æ–ø—Ä–æ—Å–∞ –∏–∑ JSON (17 –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö)
            user_answer: –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (ID, –∏—Å—Ç–æ—Ä–∏—è, —Å–æ—Å—Ç–æ—è–Ω–∏–µ)
            
        Returns:
            –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω—Å–∞–π—Ç–∞–º–∏ –∏ —á–∏—Å–ª–µ–Ω–Ω—ã–º–∏ —á–µ—Ä—Ç–∞–º–∏
        """
        
        start_time = datetime.now()
        user_id = user_context.get("user_id", "unknown")
        
        try:
            logger.info(f"üî¨ Starting comprehensive analysis for user {user_id}")
            
            # 1. –ü–û–î–ì–û–¢–û–í–ö–ê –ö–û–ù–¢–ï–ö–°–¢–ê
            enriched_context = await self._enrich_context(question_data, user_answer, user_context)
            
            # 2. –î–ï–¢–ï–ö–¶–ò–Ø –û–°–û–ë–´–• –°–ò–¢–£–ê–¶–ò–ô
            special_situation = self._detect_special_situations(user_answer, enriched_context)
            
            # 3. –í–´–ë–û–† AI –ú–û–î–ï–õ–ò 
            model_name, model_config = await self.ai_router.select_model_for_analysis(
                question_data, enriched_context
            )
            
            # 4. –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ì–õ–£–ë–ò–ù–´ –ê–ù–ê–õ–ò–ó–ê
            analysis_depth = self.config.get_analysis_depth(
                question_number=enriched_context.get("question_number", 1),
                trust_level=enriched_context.get("trust_level", 0.5),
                energy_level=enriched_context.get("energy_level", 0.5),
                fatigue_level=enriched_context.get("fatigue_level", 0.0)
            )
            
            # 5. –ü–û–õ–£–ß–ï–ù–ò–ï AI –ê–ù–ê–õ–ò–ó–ê
            ai_analysis = await self._get_ai_analysis(
                model_name, 
                model_config,
                question_data,
                user_answer, 
                enriched_context,
                analysis_depth,
                special_situation
            )
            
            # 6. –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ß–ï–†–¢ –õ–ò–ß–ù–û–°–¢–ò
            trait_analysis = await self.trait_extractor.extract_traits_from_analysis(
                ai_analysis, question_data, enriched_context
            )
            
            # 7. –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ì–û –†–ï–ó–£–õ–¨–¢–ê–¢–ê
            final_result = self._build_final_analysis(
                ai_analysis,
                trait_analysis,
                question_data,
                enriched_context,
                model_name,
                analysis_depth,
                special_situation
            )
            
            # 8. –û–ë–ù–û–í–õ–ï–ù–ò–ï –°–¢–ê–¢–ò–°–¢–ò–ö–ò
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            self._update_analysis_stats(final_result, processing_time, True)
            
            # 9. –¢–†–ï–ö–ò–ù–ì AI –ú–û–î–ï–õ–ò
            await self._track_ai_usage(model_name, model_config, final_result, processing_time)
            
            logger.info(f"‚úÖ Comprehensive analysis completed for user {user_id} in {processing_time:.0f}ms")
            return final_result
            
        except Exception as e:
            logger.error(f"‚ùå Error in comprehensive analysis for user {user_id}: {e}")
            
            # Fallback –∞–Ω–∞–ª–∏–∑
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            self._update_analysis_stats({}, processing_time, False)
            
            return await self._get_emergency_analysis(question_data, user_answer, user_context, str(e))
    
    async def _enrich_context(
        self, 
        question_data: Dict[str, Any], 
        user_answer: str, 
        user_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        
        enriched = user_context.copy()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–∞
        enriched.update({
            "answer_length": len(user_answer),
            "answer_word_count": len(user_answer.split()),
            "answer_sentence_count": user_answer.count('.') + user_answer.count('!') + user_answer.count('?'),
            "time_of_day": datetime.now().hour,
            "user_answer": user_answer,  # –î–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
            # –ò–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–∞
            "question_domain": question_data["classification"]["domain"],
            "question_depth": question_data["classification"]["depth_level"],
            "question_energy": question_data["classification"]["energy_dynamic"],
            "question_complexity": question_data["psychology"]["complexity"],
            "question_emotional_weight": question_data["psychology"]["emotional_weight"],
            "question_insight_potential": question_data["psychology"]["insight_potential"]
        })
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –¥–æ–≤–µ—Ä–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        question_number = enriched.get("question_number", 1)
        enriched["trust_level"] = min(1.0, 0.2 + (question_number / 30.0) * 0.8)
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º —ç–Ω–µ—Ä–≥–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        enriched["energy_level"] = self._estimate_user_energy(enriched)
        
        return enriched
    
    def _detect_special_situations(self, user_answer: str, context: Dict[str, Any]) -> Optional[str]:
        """
        –î–µ—Ç–µ–∫—Ü–∏—è –æ—Å–æ–±—ã—Ö —Å–∏—Ç—É–∞—Ü–∏–π —Ç—Ä–µ–±—É—é—â–∏—Ö —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞
        
        Returns:
            "crisis" | "breakthrough" | "resistance" | None
        """
        
        answer_lower = user_answer.lower()
        
        # –î–µ—Ç–µ–∫—Ü–∏—è –∫—Ä–∏–∑–∏—Å–∞
        crisis_keywords = self.config.SAFETY_RULES["crisis_keywords"]
        if any(keyword in answer_lower for keyword in crisis_keywords):
            logger.warning(f"üö® Crisis indicators detected in answer")
            return "crisis"
        
        # –î–µ—Ç–µ–∫—Ü–∏—è –ø—Ä–æ—Ä—ã–≤–∞ (–ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º –≤ –æ—Ç–≤–µ—Ç–µ)
        breakthrough_indicators = [
            "–ø–æ–Ω—è–ª", "–æ—Å–æ–∑–Ω–∞–ª", "–ø—Ä–æ—Ä—ã–≤", "–∏–Ω—Å–∞–π—Ç", "–≤–¥—Ä—É–≥ –ø–æ–Ω—è–ª",
            "—Ç–µ–ø–µ—Ä—å –≤–∏–∂—É", "–æ—Ç–∫—Ä—ã–ª–æ—Å—å", "—Å—Ç–∞–ª–æ —è—Å–Ω–æ"
        ]
        if (any(indicator in answer_lower for indicator in breakthrough_indicators) and
            len(user_answer) > 100):
            logger.info(f"üåü Breakthrough moment detected")
            return "breakthrough"
        
        # –î–µ—Ç–µ–∫—Ü–∏—è —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è  
        resistance_indicators = [
            "–Ω–µ —Ö–æ—á—É", "–Ω–µ –±—É–¥—É", "–≥–ª—É–ø—ã–π –≤–æ–ø—Ä–æ—Å", "–Ω–µ –ø–æ–Ω–∏–º–∞—é –∑–∞—á–µ–º",
            "–∫–∞–∫–∞—è —Ä–∞–∑–Ω–∏—Ü–∞", "–Ω–µ –≤–∞–∂–Ω–æ", "–±–µ–∑ –ø–æ–Ω—è—Ç–∏—è"
        ]
        if any(indicator in answer_lower for indicator in resistance_indicators):
            logger.info(f"üõ°Ô∏è Resistance detected")
            return "resistance"
        
        return None
    
    async def _get_ai_analysis(
        self,
        model_name: str,
        model_config: Dict[str, Any],
        question_data: Dict[str, Any],
        user_answer: str,
        context: Dict[str, Any],
        analysis_depth: str,
        special_situation: Optional[str]
    ) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –æ—Ç AI –º–æ–¥–µ–ª–∏"""
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø—Ä–æ–º–ø—Ç
            prompt = self._build_ai_prompt(
                model_name, analysis_depth, special_situation, 
                question_data, user_answer, context
            )
            
            # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –≤—ã–∑–æ–≤ —Ä–µ–∞–ª—å–Ω–æ–π AI API
            # –ü–æ–∫–∞ —á—Ç–æ mock –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
            ai_response = await self._call_ai_api(model_name, prompt, model_config)
            
            # –ü–∞—Ä—Å–∏–º –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            parsed_analysis = self._parse_and_validate_ai_response(ai_response, model_name)
            
            return parsed_analysis
            
        except Exception as e:
            logger.error(f"‚ùå AI analysis failed: {e}")
            
            # Fallback –Ω–∞ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å
            fallback_model, fallback_config = await self.ai_router.get_fallback_model(model_name, e)
            
            if fallback_model != "rule_based":
                return await self._get_ai_analysis(
                    fallback_model, fallback_config, question_data, 
                    user_answer, context, analysis_depth, special_situation
                )
            else:
                # –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback - –ø—Ä–∞–≤–∏–ª–∞ –±–µ–∑ AI
                return self._get_rule_based_analysis(user_answer, question_data, context)
    
    def _build_ai_prompt(
        self,
        model_name: str,
        analysis_depth: str, 
        special_situation: Optional[str],
        question_data: Dict[str, Any],
        user_answer: str,
        context: Dict[str, Any]
    ) -> str:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è AI –º–æ–¥–µ–ª–∏"""
        
        # –ë–∞–∑–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        prompt_context = {
            "question_text": question_data["text"],
            "domain": question_data["classification"]["domain"], 
            "depth_level": question_data["classification"]["depth_level"],
            "energy_dynamic": question_data["classification"]["energy_dynamic"],
            "complexity": question_data["psychology"]["complexity"],
            "emotional_weight": question_data["psychology"]["emotional_weight"],
            "insight_potential": question_data["psychology"]["insight_potential"],
            "trust_requirement": question_data["psychology"]["trust_requirement"],
            "safety_level": question_data["psychology"]["safety_level"],
            
            "user_answer": user_answer,
            "question_number": context.get("question_number", 1),
            "response_time_seconds": context.get("response_time_seconds", 30),
            "answer_length": len(user_answer),
            "time_of_day": context.get("time_of_day", datetime.now().hour),
            "current_timestamp": datetime.now().isoformat(),
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            "domain_description": self._get_domain_description(question_data["classification"]["domain"]),
            "previous_domains": context.get("previous_domains", [])
        }
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ç–∏–ø –∞–Ω–∞–ª–∏–∑–∞
        if special_situation:
            analysis_type = special_situation  # crisis, breakthrough, resistance
        else:
            analysis_type = "standard"
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ templates
        if special_situation in ["crisis", "breakthrough", "resistance"]:
            prompt = self.templates.get_prompt_for_model(model_name, special_situation, prompt_context)
        else:
            prompt = self.templates.get_depth_prompt(analysis_depth, prompt_context)
        
        return prompt
    
    async def _call_ai_api(self, model_name: str, prompt: str, model_config: Dict[str, Any]) -> str:
        """
        –í—ã–∑–æ–≤ AI API (—Ä–µ–∞–ª—å–Ω—ã–π OpenAI/Anthropic)

        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (gpt-4o, gpt-4o-mini, claude-3.5-sonnet)
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            model_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ (timeout, temperature, etc.)

        Returns:
            JSON —Å—Ç—Ä–æ–∫–∞ —Å –∞–Ω–∞–ª–∏–∑–æ–º –æ—Ç AI
        """

        start_time = datetime.now()

        try:
            if model_name == "claude-3.5-sonnet":
                # Anthropic Claude
                if not self.anthropic_client:
                    raise ValueError("Anthropic client not initialized - check ANTHROPIC_API_KEY")

                response = await self.anthropic_client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=model_config.get("max_tokens", 2000),
                    temperature=model_config.get("temperature", 0.7),
                    messages=[{"role": "user", "content": prompt}]
                )

                ai_response = response.content[0].text

            elif model_name in ["gpt-4o", "gpt-4o-mini"]:
                # OpenAI GPT
                if not self.openai_client:
                    raise ValueError("OpenAI client not initialized - check OPENAI_API_KEY")

                response = await self.openai_client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {
                            "role": "system",
                            "content": "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Å–∏—Ö–æ–ª–æ–≥-–∞–Ω–∞–ª–∏—Ç–∏–∫. –í–æ–∑–≤—Ä–∞—â–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."
                        },
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    temperature=model_config.get("temperature", 0.7),
                    max_tokens=model_config.get("max_tokens", 2000),
                    response_format={"type": "json_object"}  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON
                )

                ai_response = response.choices[0].message.content

            else:
                raise ValueError(f"Unknown model: {model_name}")

            # –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è
            processing_time = (datetime.now() - start_time).total_seconds() * 1000

            logger.info(f"ü§ñ AI analysis completed using {model_name} in {processing_time:.0f}ms")

            return ai_response

        except Exception as e:
            logger.error(f"‚ùå AI API call failed for {model_name}: {e}")
            # Fallback –Ω–∞ —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            raise
    
    def _parse_and_validate_ai_response(self, ai_response: str, model_name: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ AI"""
        
        try:
            # –ü–∞—Ä—Å–∏–º JSON
            parsed = json.loads(ai_response)
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
            required_fields = ["core_analysis", "traits"]
            if self.templates.validate_response(ai_response, required_fields):
                return parsed
            else:
                logger.warning(f"‚ö†Ô∏è AI response validation failed for {model_name}")
                return self._fix_malformed_response(parsed)
                
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå JSON parsing failed for {model_name}: {e}")
            return self._extract_from_malformed_json(ai_response)
    
    def _fix_malformed_response(self, parsed: Dict) -> Dict[str, Any]:
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ AI"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        if "core_analysis" not in parsed:
            parsed["core_analysis"] = {
                "emotional_state": {"primary": "neutral", "valence": 0.0, "arousal": 0.0},
                "insights": {"main": "–ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–∏—á–Ω–æ –¥–æ—Å—Ç—É–ø–µ–Ω"}
            }
        
        if "traits" not in parsed:
            parsed["traits"] = {
                "big_five": {trait: 0.5 for trait in self.config.BIG_FIVE_TRAITS}
            }
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º Big Five –∑–Ω–∞—á–µ–Ω–∏—è
        big_five = parsed.get("traits", {}).get("big_five", {})
        for trait in self.config.BIG_FIVE_TRAITS:
            if trait not in big_five or not (0 <= big_five[trait] <= 1):
                big_five[trait] = 0.5
        
        parsed["_fixed_by_validator"] = True
        return parsed
    
    def _build_final_analysis(
        self,
        ai_analysis: Dict[str, Any],
        trait_analysis: Dict[str, Any], 
        question_data: Dict[str, Any],
        context: Dict[str, Any],
        model_name: str,
        analysis_depth: str,
        special_situation: Optional[str]
    ) -> Dict[str, Any]:
        """–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        
        return {
            # –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            "analysis_version": "2.0",
            "created_at": datetime.now().isoformat(),
            "processing_metadata": {
                "model_used": model_name,
                "analysis_depth": analysis_depth,
                "special_situation": special_situation,
                "question_domain": question_data["classification"]["domain"],
                "question_number": context.get("question_number", 1)
            },
            
            # –û—Å–Ω–æ–≤–Ω–æ–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
            "psychological_analysis": {
                "insights": ai_analysis.get("core_analysis", {}).get("insights", {}),
                "emotional_assessment": ai_analysis.get("core_analysis", {}).get("emotional_state", {}),
                "behavioral_patterns": ai_analysis.get("patterns", []),
                "growth_indicators": ai_analysis.get("growth_indicators", [])
            },
            
            # –ß–∏—Å–ª–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ —á–µ—Ä—Ç
            "personality_traits": trait_analysis["traits"],
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫–∞—á–µ—Å—Ç–≤–∞
            "quality_metadata": {
                "trait_confidence": trait_analysis["assessment_metadata"]["confidence_scores"],
                "overall_reliability": trait_analysis["assessment_metadata"]["reliability_indicators"]["overall_reliability"],
                "data_completeness": self._assess_data_completeness(ai_analysis, trait_analysis),
                "needs_validation": trait_analysis["assessment_metadata"]["reliability_indicators"]["needs_validation"]
            },
            
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Ä–æ—É—Ç–µ—Ä–∞
            "router_recommendations": ai_analysis.get("recommendations", {}),
            
            # –î–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
            "personality_summary": self._generate_personality_summary(ai_analysis, trait_analysis, context),
            
            # –°–ª—É–∂–µ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            "debug_info": {
                "raw_ai_response_length": len(str(ai_analysis)),
                "trait_extraction_successful": True,
                "fallback_used": context.get("fallback_used", False),
                "processing_notes": []
            }
        }
    
    def _generate_personality_summary(
        self, 
        ai_analysis: Dict[str, Any], 
        trait_analysis: Dict[str, Any],
        context: Dict[str, Any]
    ) -> Dict[str, str]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ summary –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–∞–∑–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
        """
        
        insights = ai_analysis.get("core_analysis", {}).get("insights", {})
        main_insight = insights.get("main", "")
        traits = trait_analysis["traits"]["big_five"]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞—Ä—Ö–µ—Ç–∏–ø –ª–∏—á–Ω–æ—Å—Ç–∏
        archetype = self._determine_personality_archetype(traits)
        
        # –£—Ä–æ–≤–µ–Ω—å 1: Nano-summary (50 —Å–∏–º–≤–æ–ª–æ–≤)
        dominant_traits = sorted(traits.items(), key=lambda x: x[1], reverse=True)[:2]
        nano = f"{archetype} ({dominant_traits[0][0]}: {dominant_traits[0][1]:.1f})"
        
        # –£—Ä–æ–≤–µ–Ω—å 2: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å
        structured = {
            "archetype": archetype,
            "core_traits": [f"{trait}: {value:.2f}" for trait, value in dominant_traits[:3]],
            "communication_style": self._infer_communication_style(traits, ai_analysis),
            "energy_signature": self._infer_energy_pattern(context),
            "growth_edge": insights.get("growth_edge", "—Ä–∞–∑–≤–∏—Ç–∏–µ —Å–∞–º–æ—Å–æ–∑–Ω–∞–Ω–∏—è")
        }
        
        # –£—Ä–æ–≤–µ–Ω—å 3: Narrative (200-300 —Å–ª–æ–≤)
        narrative = f"""
        {main_insight}
        
        –û—Å–Ω–æ–≤–Ω—ã–µ —á–µ—Ä—Ç—ã: –≤—ã—Å–æ–∫–∞—è {dominant_traits[0][0]} ({dominant_traits[0][1]:.2f}) –∏ 
        {dominant_traits[1][0]} ({dominant_traits[1][1]:.2f}), —á—Ç–æ —Å–æ–∑–¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω 
        –ª–∏—á–Ω–æ—Å—Ç–∏. {insights.get('patterns', [''])[0] if insights.get('patterns') else ''}
        
        –°—Ç–∏–ª—å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è: {structured['communication_style']}.
        –ó–æ–Ω–∞ —Ä–æ—Å—Ç–∞: {structured['growth_edge']}.
        """
        
        # –£—Ä–æ–≤–µ–Ω—å 4: Embedding prompt
        embedding_prompt = f"""
        {archetype}, {' '.join([f'{trait} {value:.1f}' for trait, value in traits.items()])},
        {structured['communication_style']}, {structured['energy_signature']}
        """
        
        return {
            "nano": nano[:50],  # –°—Ç—Ä–æ–≥–æ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            "structured": json.dumps(structured, ensure_ascii=False),
            "narrative": narrative.strip(),
            "embedding_prompt": embedding_prompt
        }
    
    def _determine_personality_archetype(self, big_five_traits: Dict[str, float]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞—Ä—Ö–µ—Ç–∏–ø–∞ –ª–∏—á–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ Big Five"""

        o, c, e, a, n = [big_five_traits[trait] for trait in ["openness", "conscientiousness", "extraversion", "agreeableness", "neuroticism"]]

        # –ü—Ä–æ—Å—Ç–∞—è –ª–æ–≥–∏–∫–∞ –∞—Ä—Ö–µ—Ç–∏–ø–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏—Ö —á–µ—Ä—Ç
        if o > 0.7 and e < 0.5:
            return "–ú—É–¥—Ä—ã–π –°–æ–∑–µ—Ä—Ü–∞—Ç–µ–ª—å"
        elif o > 0.7 and e > 0.6:
            return "–í–¥–æ—Ö–Ω–æ–≤–ª—è—é—â–∏–π –¢–≤–æ—Ä–µ—Ü"
        elif c > 0.7 and a > 0.6:
            return "–ù–∞–¥–µ–∂–Ω—ã–π –û—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä"
        elif e > 0.7 and a > 0.6:
            return "–¢–µ–ø–ª—ã–π –ö–æ–º–º—É–Ω–∏–∫–∞—Ç–æ—Ä"
        elif o > 0.6 and c > 0.6:
            return "–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å"
        elif a > 0.7:
            return "–ß—É—Ç–∫–∏–π –ü–æ–º–æ—â–Ω–∏–∫"
        elif c > 0.6:
            return "–¶–µ–ª–µ—É—Å—Ç—Ä–µ–º–ª–µ–Ω–Ω—ã–π –î–µ—è—Ç–µ–ª—å"
        elif e > 0.6:
            return "–°–æ—Ü–∏–∞–ª—å–Ω—ã–π –≠–Ω—Ç—É–∑–∏–∞—Å—Ç"
        elif o > 0.6:
            return "–ò—â—É—â–∏–π –ù–æ–≤–æ–≥–æ"
        else:
            return "–ì–∞—Ä–º–æ–Ω–∏—á–Ω–∞—è –õ–∏—á–Ω–æ—Å—Ç—å"

    def _infer_communication_style(self, big_five_traits: Dict[str, float], ai_analysis: Dict[str, Any]) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∏–ª—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–µ—Ä—Ç –ª–∏—á–Ω–æ—Å—Ç–∏

        Args:
            big_five_traits: Big Five —á–µ—Ä—Ç—ã –ª–∏—á–Ω–æ—Å—Ç–∏
            ai_analysis: –†–µ–∑—É–ª—å—Ç–∞—Ç AI –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            –û–ø–∏—Å–∞–Ω–∏–µ —Å—Ç–∏–ª—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏
        """
        e = big_five_traits.get("extraversion", 0.5)
        a = big_five_traits.get("agreeableness", 0.5)
        o = big_five_traits.get("openness", 0.5)
        c = big_five_traits.get("conscientiousness", 0.5)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∏–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —á–µ—Ä—Ç
        if e > 0.7 and a > 0.6:
            return "–æ—Ç–∫—Ä—ã—Ç—ã–π –∏ —ç–º–ø–∞—Ç–∏—á–Ω—ã–π, –ª–µ–≥–∫–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–∞–∫—Ç"
        elif e > 0.7 and o > 0.6:
            return "—ç–Ω–µ—Ä–≥–∏—á–Ω—ã–π –∏ —Ç–≤–æ—Ä—á–µ—Å–∫–∏–π, –≤–¥–æ—Ö–Ω–æ–≤–ª—è–µ—Ç —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–æ–≤"
        elif e < 0.4 and o > 0.6:
            return "–≤–¥—É–º—á–∏–≤—ã–π –∏ –≥–ª—É–±–æ–∫–∏–π, –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ –±–µ—Å–µ–¥—ã"
        elif c > 0.7 and a > 0.6:
            return "—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–π –∫ –¥–µ—Ç–∞–ª—è–º"
        elif a > 0.7:
            return "–¥–∏–ø–ª–æ–º–∞—Ç–∏—á–Ω—ã–π –∏ —á—É—Ç–∫–∏–π, –Ω–∞—Ö–æ–¥–∏—Ç –æ–±—â–∏–π —è–∑—ã–∫ —Å —Ä–∞–∑–Ω—ã–º–∏ –ª—é–¥—å–º–∏"
        elif e > 0.6:
            return "–∞–∫—Ç–∏–≤–Ω—ã–π –∏ –æ–±—â–∏—Ç–µ–ª—å–Ω—ã–π, –ª–µ–≥–∫–æ –≤–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –¥–∏–∞–ª–æ–≥"
        elif o > 0.6:
            return "–ª—é–±–æ–∑–Ω–∞—Ç–µ–ª—å–Ω—ã–π –∏ –æ—Ç–∫—Ä—ã—Ç—ã–π –Ω–æ–≤—ã–º –∏–¥–µ—è–º"
        else:
            return "—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏ –≥–∏–±–∫–∏–π –≤ –æ–±—â–µ–Ω–∏–∏"

    def _infer_energy_pattern(self, context: Dict[str, Any]) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

        Args:
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

        Returns:
            –û–ø–∏—Å–∞–Ω–∏–µ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        """
        energy_level = context.get("energy_level", 0.5)
        engagement = context.get("engagement_level", 0.5)
        fatigue = context.get("fatigue_level", 0.0)

        if fatigue > 0.6:
            return "–Ω–µ–æ–±—Ö–æ–¥–∏–º –æ—Ç–¥—ã—Ö –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ"
        elif energy_level > 0.7 and engagement > 0.7:
            return "–≤—ã—Å–æ–∫–∞—è —ç–Ω–µ—Ä–≥–∏—è –∏ –≤–æ–≤–ª–µ—á—ë–Ω–Ω–æ—Å—Ç—å"
        elif energy_level > 0.6:
            return "—Å—Ç–∞–±–∏–ª—å–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è"
        elif energy_level < 0.4:
            return "–Ω–∏–∑–∫–∏–π —É—Ä–æ–≤–µ–Ω—å —ç–Ω–µ—Ä–≥–∏–∏, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞"
        else:
            return "—É–º–µ—Ä–µ–Ω–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è"
    
    def _get_rule_based_analysis(self, user_answer: str, question_data: Dict, context: Dict) -> Dict[str, Any]:
        """–ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ AI (–ø–æ—Å–ª–µ–¥–Ω–∏–π fallback)"""
        
        logger.warning("üîÑ Using rule-based analysis (no AI available)")
        
        # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞—Ö –∏ —ç–≤—Ä–∏—Å—Ç–∏–∫–∞—Ö
        answer_lower = user_answer.lower()
        answer_length = len(user_answer)
        
        # –ü—Ä–æ—Å—Ç–∞—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        if any(word in answer_lower for word in ["—Ä–∞–¥", "—Å—á–∞—Å—Ç–ª–∏–≤", "—Ö–æ—Ä–æ—à–æ", "–æ—Ç–ª–∏—á–Ω–æ"]):
            emotional_state = "positive"
            valence = 0.7
        elif any(word in answer_lower for word in ["–≥—Ä—É—Å—Ç–Ω–æ", "–ø–ª–æ—Ö–æ", "—Å–ª–æ–∂–Ω–æ", "—Ç—è–∂–µ–ª–æ"]):
            emotional_state = "negative"
            valence = -0.3
        else:
            emotional_state = "neutral"
            valence = 0.0
        
        # –ü—Ä–æ—Å—Ç—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è Big Five
        simple_traits = {}
        for trait in self.config.BIG_FIVE_TRAITS:
            simple_traits[trait] = self.trait_extractor._estimate_trait_from_text(
                trait, user_answer, question_data
            )
        
        return {
            "analysis_version": "2.0",
            "timestamp": datetime.now().isoformat(),
            "core_analysis": {
                "emotional_state": {
                    "primary": emotional_state,
                    "valence": valence,
                    "arousal": 0.3
                },
                "insights": {
                    "main": f"–ê–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–≤—Ä–∏—Å—Ç–∏–∫. –û—Ç–≤–µ—Ç –¥–ª–∏–Ω–æ–π {answer_length} —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç {emotional_state} –Ω–∞—Å—Ç—Ä–æ–π.",
                    "note": "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ AI"
                }
            },
            "traits": {
                "big_five": simple_traits
            },
            "meta_assessment": {
                "confidence": 0.3,  # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                "method": "rule_based",
                "needs_ai_reanalysis": True
            },
            "_is_fallback": True
        }
    
    def _assess_data_completeness(self, ai_analysis: Dict, trait_analysis: Dict) -> float:
        """–û—Ü–µ–Ω–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –¥–∞–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞"""
        
        completeness_score = 0.0
        total_checks = 0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        checks = [
            ("emotional_state" in ai_analysis.get("core_analysis", {}), 0.2),
            ("insights" in ai_analysis.get("core_analysis", {}), 0.2),
            ("big_five" in trait_analysis.get("traits", {}), 0.3),
            ("confidence_scores" in trait_analysis.get("assessment_metadata", {}), 0.1),
            ("recommendations" in ai_analysis, 0.2)
        ]
        
        for check, weight in checks:
            total_checks += weight
            if check:
                completeness_score += weight
        
        return round(completeness_score / total_checks if total_checks > 0 else 0.0, 2)
    
    def _update_analysis_stats(self, result: Dict, processing_time_ms: float, success: bool):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞"""
        
        self.analysis_stats["total_analyses"] += 1
        
        if success:
            self.analysis_stats["successful_analyses"] += 1
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è
            old_avg = self.analysis_stats["avg_processing_time_ms"]
            old_count = self.analysis_stats["successful_analyses"] - 1
            
            if old_count > 0:
                new_avg = (old_avg * old_count + processing_time_ms) / (old_count + 1)
            else:
                new_avg = processing_time_ms
                
            self.analysis_stats["avg_processing_time_ms"] = round(new_avg, 1)
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è
            if result.get("processing_metadata", {}).get("special_situation") == "crisis":
                self.analysis_stats["crisis_detections"] += 1
            elif result.get("processing_metadata", {}).get("special_situation") == "breakthrough":
                self.analysis_stats["breakthrough_moments"] += 1
    
    async def _track_ai_usage(self, model_name: str, model_config: Dict, result: Dict, processing_time_ms: float):
        """–¢—Ä–µ–∫–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AI –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–∞–ª–∏–∑–∞
        quality_score = result.get("quality_metadata", {}).get("overall_reliability", 0.5)
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Å—Ç–æ–∏–º–æ—Å—Ç—å (–ø—Ä–∏–º–µ—Ä–Ω–æ)
        tokens_used = len(str(result)) // 4  # –ì—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
        model_cost_per_token = self.config.AI_MODEL_SETTINGS[model_name].get("cost_per_token", 0.00001)
        estimated_cost = tokens_used * model_cost_per_token
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ —Ä–æ—É—Ç–µ—Ä –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.ai_router.track_usage(model_name, estimated_cost, quality_score, int(processing_time_ms))

    def _estimate_user_energy(self, context: Dict[str, Any]) -> float:
        """
        –û—Ü–µ–Ω–∫–∞ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

        Args:
            context: –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å –¥–∞–Ω–Ω—ã–º–∏ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ

        Returns:
            float –æ—Ç 0.0 –¥–æ 1.0 (0.0 = –Ω–∏–∑–∫–∞—è —ç–Ω–µ—Ä–≥–∏—è, 1.0 = –≤—ã—Å–æ–∫–∞—è)
        """

        energy_score = 0.5  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞

        # 1. –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–æ–≤ (–±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ = –≤—ã—à–µ —ç–Ω–µ—Ä–≥–∏—è)
        answer_length = context.get("answer_length", 50)
        if answer_length > 200:
            energy_score += 0.2
        elif answer_length < 30:
            energy_score -= 0.2

        # 2. –ê–Ω–∞–ª–∏–∑ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–æ–≤ (–±—ã—Å—Ç—Ä—ã–µ = –≤—ã—à–µ —ç–Ω–µ—Ä–≥–∏—è)
        response_time = context.get("response_time_seconds", 60)
        if response_time < 30:
            energy_score += 0.1
        elif response_time > 120:
            energy_score -= 0.1

        # 3. –ü—Ä–æ–≥—Ä–µ—Å—Å –≤ –æ–Ω–±–æ—Ä–¥–∏–Ω–≥–µ (—É—Å—Ç–∞–ª–æ—Å—Ç—å –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ—Ç—Å—è)
        question_number = context.get("question_number", 1)
        fatigue_factor = min(0.3, question_number / 100)  # Max 0.3 —É—Å—Ç–∞–ª–æ—Å—Ç–∏
        energy_score -= fatigue_factor

        # 4. Fatigue level –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        fatigue_level = context.get("fatigue_level", 0.0)
        energy_score -= fatigue_level * 0.3

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0.0, 1.0]
        return max(0.0, min(1.0, energy_score))

    def _get_domain_description(self, domain: str) -> str:
        """
        –ü–æ–ª—É—á–∏—Ç—å —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–º–µ–Ω–∞

        Args:
            domain: –ö–æ–¥ –¥–æ–º–µ–Ω–∞ (IDENTITY, EMOTIONS, etc.)

        Returns:
            –û–ø–∏—Å–∞–Ω–∏–µ –¥–æ–º–µ–Ω–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
        """

        domain_descriptions = {
            "IDENTITY": "–ò–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å –∏ —Å–∞–º–æ–ø–æ–∑–Ω–∞–Ω–∏–µ",
            "EMOTIONS": "–≠–º–æ—Ü–∏–∏ –∏ —á—É–≤—Å—Ç–≤–∞",
            "RELATIONSHIPS": "–û—Ç–Ω–æ—à–µ–Ω–∏—è –∏ —Å–≤—è–∑–∏",
            "VALUES": "–¶–µ–Ω–Ω–æ—Å—Ç–∏ –∏ —É–±–µ–∂–¥–µ–Ω–∏—è",
            "GOALS": "–¶–µ–ª–∏ –∏ —Å—Ç—Ä–µ–º–ª–µ–Ω–∏—è",
            "FEARS": "–°—Ç—Ä–∞—Ö–∏ –∏ –æ–ø–∞—Å–µ–Ω–∏—è",
            "GROWTH": "–†–æ—Å—Ç –∏ —Ä–∞–∑–≤–∏—Ç–∏–µ",
            "PAST": "–ü—Ä–æ—à–ª–æ–µ –∏ –æ–ø—ã—Ç",
            "FUTURE": "–ë—É–¥—É—â–µ–µ –∏ –º–µ—á—Ç—ã",
            "WORK": "–†–∞–±–æ—Ç–∞ –∏ –ø—Ä–∏–∑–≤–∞–Ω–∏–µ",
            "CREATIVITY": "–¢–≤–æ—Ä—á–µ—Å—Ç–≤–æ –∏ —Å–∞–º–æ–≤—ã—Ä–∞–∂–µ–Ω–∏–µ",
            "BODY": "–¢–µ–ª–æ –∏ –∑–¥–æ—Ä–æ–≤—å–µ",
            "SPIRITUALITY": "–î—É—Ö–æ–≤–Ω–æ—Å—Ç—å –∏ —Å–º—ã—Å–ª—ã"
        }

        return domain_descriptions.get(domain, f"–î–æ–º–µ–Ω: {domain}")

    async def _get_emergency_analysis(
        self,
        question_data: Dict[str, Any],
        user_answer: str,
        user_context: Dict[str, Any],
        error_message: str
    ) -> Dict[str, Any]:
        """
        Emergency fallback –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏ —Å–±–æ–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ pipeline

        Args:
            question_data: –î–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å–∞
            user_answer: –û—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            error_message: –°–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ

        Returns:
            –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –Ω–æ –≤–∞–ª–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        """

        user_id = user_context.get("user_id", "unknown")

        logger.warning(f"üö® Using emergency analysis for user {user_id} due to: {error_message}")

        # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤–∞–ª–∏–¥–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        return {
            "user_id": user_id,
            "question_id": question_data.get("id", "unknown"),

            # Personality traits (–≤–µ—Ä—Å–∏—è 2.0)
            "personality_traits": {
                "version": "2.0",
                "big_five": {
                    "openness": {"score": 0.5, "confidence": 0.2},
                    "conscientiousness": {"score": 0.5, "confidence": 0.2},
                    "extraversion": {"score": 0.5, "confidence": 0.2},
                    "agreeableness": {"score": 0.5, "confidence": 0.2},
                    "neuroticism": {"score": 0.5, "confidence": 0.2}
                },
                "timestamp": datetime.now().isoformat()
            },

            # –ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ insights
            "psychological_analysis": {
                "insights": {
                    "primary": "–°–ø–∞—Å–∏–±–æ –∑–∞ –≤–∞—à –æ—Ç–≤–µ—Ç. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é...",
                    "secondary": []
                },
                "emotional_assessment": {
                    "primary": "neutral",
                    "intensity": 0.3
                }
            },

            # Router recommendations (–ø—É—Å—Ç—ã–µ)
            "router_recommendations": {},

            # Processing metadata
            "processing_metadata": {
                "model_used": "emergency_handler",
                "processing_time_ms": 0,
                "timestamp": datetime.now().isoformat(),
                "special_situation": None
            },

            # Quality metadata
            "quality_metadata": {
                "overall_reliability": 0.3,
                "data_completeness": 0.5,
                "fatigue_indicators": 0.0
            },

            # Debug info
            "debug_info": {
                "is_emergency_analysis": True,
                "original_error": error_message
            },

            # ‚úÖ –î–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π personality_summary
            "personality_summary": {
                "nano": user_answer[:50] if user_answer else "No answer",
                "structured": "{}",
                "narrative": f"Emergency analysis. Answer: {user_answer[:200] if user_answer else 'No answer'}",
                "embedding_prompt": user_answer[:500] if user_answer else "No answer provided"
            },

            # Analysis version
            "analysis_version": "emergency_fallback_v1"
        }

    async def get_analysis_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        
        success_rate = 0
        if self.analysis_stats["total_analyses"] > 0:
            success_rate = (self.analysis_stats["successful_analyses"] / self.analysis_stats["total_analyses"]) * 100
        
        return {
            **self.analysis_stats,
            "success_rate_percent": round(success_rate, 1),
            "ai_router_stats": self.ai_router.get_usage_report()
        }