"""
Trait Extractor - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–µ—Ä—Ç –ª–∏—á–Ω–æ—Å—Ç–∏

üß¨ –ü–†–ò–ù–¶–ò–ü: –ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω–∞—è –º–æ–¥–µ–ª—å –ª–∏—á–Ω–æ—Å—Ç–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π
üìä –¢–û–ß–ù–û–°–¢–¨: 2 –∑–Ω–∞–∫–∞ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π + —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
üéØ –ê–î–ê–ü–¢–ò–í–ù–û–°–¢–¨: –†–∞–∑–Ω—ã–µ —á–µ—Ä—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤
"""

import logging
import json
from typing import Dict, Any, List, Optional
from datetime import datetime
import numpy as np

from .analysis_config import AnalysisConfig

logger = logging.getLogger(__name__)

class TraitExtractor:
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ —á–µ—Ä—Ç –ª–∏—á–Ω–æ—Å—Ç–∏ –∏–∑ AI –∞–Ω–∞–ª–∏–∑–∞
    
    –ú–Ω–æ–≥–æ—Å–ª–æ–π–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
    - Big Five (—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç)
    - –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —á–µ—Ä—Ç—ã (–º–µ–Ω—è—é—Ç—Å—è —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º)
    - –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —á–µ—Ä—Ç—ã (–±—ã—Å—Ç—Ä–æ –º–µ–Ω—è—é—â–∏–µ—Å—è —Å–æ—Å—Ç–æ—è–Ω–∏—è)  
    - –î–æ–º–µ–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –æ–±–ª–∞—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞)
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞"""
        self.config = AnalysisConfig()
        
        # –ö—ç—à –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ (–ø–æ–∫–∞ –Ω–µ –Ω–∞–∫–æ–ø–∏–ª–∏—Å—å –¥–∞–Ω–Ω—ã–µ)
        self.trait_cache = {}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        self.population_stats = {
            "mean": {},
            "std": {},
            "sample_size": 0
        }
        
        logger.info("üìä TraitExtractor initialized with multilayer personality model")
    
    async def extract_traits_from_analysis(
        self, 
        ai_analysis: Dict[str, Any],
        question_metadata: Dict[str, Any],
        user_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —á–µ—Ä—Ç –∏–∑ AI –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            ai_analysis: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç AI
            question_metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å–∞ (17 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
            user_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —Å–µ—Å—Å–∏–∏
            
        Returns:
            –ü–æ–ª–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —á–µ—Ä—Ç –ª–∏—á–Ω–æ—Å—Ç–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        
        try:
            logger.info(f"üìä Extracting traits from AI analysis (model: {user_context.get('model_used', 'unknown')})")
            
            # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—ã–µ —á–µ—Ä—Ç—ã (Big Five)
            big_five = self._extract_big_five(ai_analysis, question_metadata, user_context)
            
            # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —á–µ—Ä—Ç—ã
            dynamic_traits = self._extract_dynamic_traits(ai_analysis, question_metadata)
            
            # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —á–µ—Ä—Ç—ã (—Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ)
            adaptive_traits = self._extract_adaptive_traits(ai_analysis, user_context)
            
            # 4. –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–º–µ–Ω–Ω—ã–µ —á–µ—Ä—Ç—ã (–µ—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ)
            domain_traits = self._extract_domain_specific_traits(ai_analysis, question_metadata)
            
            # 5. –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
            assessment_metadata = self._generate_assessment_metadata(
                ai_analysis, question_metadata, user_context
            )
            
            # 6. –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è
            normalized_traits = self._normalize_and_validate_traits({
                "big_five": big_five,
                "dynamic_traits": dynamic_traits,
                "adaptive_traits": adaptive_traits,
                "domain_specific": domain_traits
            })
            
            # 7. –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            timestamp = datetime.now().isoformat()
            result = {
                "version": "2.0",
                "timestamp": timestamp,  # –î–ª—è DB constraint
                "extracted_at": timestamp,  # –î–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
                "big_five": big_five,  # –î–ª—è DB constraint (–ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø)
                "traits": normalized_traits,
                "assessment_metadata": assessment_metadata,
                "extraction_context": {
                    "question_domain": question_metadata["classification"]["domain"],
                    "question_depth": question_metadata["classification"]["depth_level"],
                    "analysis_model": user_context.get("model_used"),
                    "user_question_number": user_context.get("question_number", 1)
                }
            }
            
            logger.info(f"‚úÖ Traits extracted successfully: {len(big_five)} Big Five + {len(dynamic_traits)} dynamic + {len(domain_traits)} domain-specific")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error extracting traits: {e}")
            return self._get_fallback_traits(question_metadata, user_context)
    
    def _extract_big_five(
        self, 
        ai_analysis: Dict[str, Any], 
        question_metadata: Dict[str, Any],
        user_context: Dict[str, Any]
    ) -> Dict[str, float]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ Big Five —á–µ—Ä—Ç"""
        
        big_five_traits = {}
        
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ AI
        ai_traits = ai_analysis.get("traits", {}).get("big_five", {})
        
        for trait in self.config.BIG_FIVE_TRAITS:
            # –ï—Å–ª–∏ AI –¥–∞–ª –æ—Ü–µ–Ω–∫—É - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
            if trait in ai_traits and isinstance(ai_traits[trait], (int, float)):
                raw_value = float(ai_traits[trait])
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
                normalized_value = self._apply_contextual_normalization(
                    raw_value, trait, question_metadata, user_context
                )
                
                big_five_traits[trait] = round(normalized_value, 2)
                
            else:
                # Fallback - —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç–∞
                estimated_value = self._estimate_trait_from_text(
                    trait, 
                    user_context.get("user_answer", ""),
                    question_metadata
                )
                big_five_traits[trait] = round(estimated_value, 2)
        
        return big_five_traits
    
    def _extract_dynamic_traits(self, ai_analysis: Dict[str, Any], question_metadata: Dict[str, Any]) -> Dict[str, float]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏—Ö —á–µ—Ä—Ç"""
        
        dynamic_traits = {}
        ai_dynamic = ai_analysis.get("traits", {}).get("dynamic_traits", {})
        
        for trait in self.config.DYNAMIC_TRAITS:
            if trait in ai_dynamic:
                dynamic_traits[trait] = round(float(ai_dynamic[trait]), 2)
            else:
                # –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                estimated = self._estimate_dynamic_trait(trait, ai_analysis, question_metadata)
                dynamic_traits[trait] = round(estimated, 2)
        
        return dynamic_traits
    
    def _extract_adaptive_traits(self, ai_analysis: Dict[str, Any], user_context: Dict[str, Any]) -> Dict[str, float]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö —á–µ—Ä—Ç (—Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ)"""
        
        adaptive_traits = {}
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ –º–µ—Ç–∞-–æ—Ü–µ–Ω–∫–∏ AI
        meta_assessment = ai_analysis.get("meta_assessment", {})
        
        for trait in self.config.ADAPTIVE_TRAITS:
            if trait in meta_assessment:
                adaptive_traits[trait] = round(float(meta_assessment[trait]), 2)
            else:
                # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                estimated = self._estimate_adaptive_trait(trait, ai_analysis, user_context)
                adaptive_traits[trait] = round(estimated, 2)
        
        return adaptive_traits
    
    def _extract_domain_specific_traits(self, ai_analysis: Dict[str, Any], question_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–º–µ–Ω–Ω—ã—Ö —á–µ—Ä—Ç"""
        
        domain = question_metadata["classification"]["domain"]
        domain_traits = {}
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–µ—Ä—Ç—ã –¥–ª—è –¥–æ–º–µ–Ω–∞
        relevant_traits = self.config.DOMAIN_SPECIFIC_TRAITS.get(domain, [])
        
        if relevant_traits:
            ai_domain_traits = ai_analysis.get("traits", {}).get("domain_specific", {}).get(domain, {})
            
            domain_traits[domain] = {}
            
            for trait in relevant_traits:
                if trait in ai_domain_traits:
                    domain_traits[domain][trait] = round(float(ai_domain_traits[trait]), 2)
                else:
                    # –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –¥–æ–º–µ–Ω–∞
                    estimated = self._estimate_domain_trait(trait, domain, ai_analysis)
                    domain_traits[domain][trait] = round(estimated, 2)
        
        return domain_traits
    
    def _apply_contextual_normalization(
        self, 
        raw_value: float, 
        trait_name: str,
        question_metadata: Dict[str, Any],
        user_context: Dict[str, Any]
    ) -> float:
        """
        –ü—Ä–∏–º–µ–Ω–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –æ—Ü–µ–Ω–∫–∏
        
        Args:
            raw_value: –°—ã—Ä–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç AI (0-1)
            trait_name: –ù–∞–∑–≤–∞–Ω–∏–µ —á–µ—Ä—Ç—ã
            question_metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å–∞
            user_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        """
        
        depth_level = question_metadata["classification"]["depth_level"]
        energy_dynamic = question_metadata["classification"]["energy_dynamic"]
        time_of_day = user_context.get("time_of_day", 12)
        
        normalized = raw_value
        
        # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –ø–æ –≥–ª—É–±–∏–Ω–µ –≤–æ–ø—Ä–æ—Å–∞
        if depth_level == "SHADOW":
            # –ù–∞ –≥–ª—É–±–æ–∫–∏—Ö —É—Ä–æ–≤–Ω—è—Ö –ª—é–¥–∏ –±–æ–ª–µ–µ –æ—Ç–∫—Ä—ã—Ç—ã
            normalized *= 0.95  # –ö–æ–º–ø–µ–Ω—Å–∏—Ä—É–µ–º –∑–∞–≤—ã—à–µ–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        elif depth_level == "SURFACE":
            # –ù–∞ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –ª—é–¥–∏ –±–æ–ª–µ–µ –æ—Å—Ç–æ—Ä–æ–∂–Ω—ã
            normalized *= 1.05  # –ö–æ–º–ø–µ–Ω—Å–∏—Ä—É–µ–º –∑–∞–Ω–∏–∂–µ–Ω–Ω—ã–µ
        
        # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –ø–æ —ç–Ω–µ—Ä–≥–∏–∏
        if energy_dynamic == "HEAVY" and trait_name in ["neuroticism"]:
            # –í —Ç—è–∂–µ–ª—ã–µ –º–æ–º–µ–Ω—Ç—ã neuroticism –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–≤—ã—à–µ–Ω
            normalized *= 0.9
        elif energy_dynamic == "OPENING" and trait_name in ["openness"]:
            # –í –≤–¥–æ—Ö–Ω–æ–≤–ª—è—é—â–∏–µ –º–æ–º–µ–Ω—Ç—ã openness –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–≤—ã—à–µ–Ω  
            normalized *= 0.95
        
        # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏ —Å—É—Ç–æ–∫
        if time_of_day >= 20:  # –í–µ—á–µ—Ä–æ–º
            if trait_name == "conscientiousness":
                normalized *= 0.95  # –í–µ—á–µ—Ä–æ–º –º–µ–Ω–µ–µ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω—ã
        elif time_of_day <= 8:  # –£—Ç—Ä–æ–º  
            if trait_name == "extraversion":
                normalized *= 0.9  # –£—Ç—Ä–æ–º –º–µ–Ω–µ–µ —Å–æ—Ü–∏–∞–ª—å–Ω—ã
        
        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-1
        return max(0.0, min(1.0, normalized))
    
    def _estimate_trait_from_text(self, trait_name: str, answer_text: str, question_metadata: Dict) -> float:
        """–≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞ (fallback)"""
        
        if not answer_text:
            return 0.5  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        
        answer_lower = answer_text.lower()
        answer_length = len(answer_text)
        
        # –≠–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —á–µ—Ä—Ç—ã Big Five
        if trait_name == "openness":
            # –ú–∞—Ä–∫–µ—Ä—ã –æ—Ç–∫—Ä—ã—Ç–æ—Å—Ç–∏: –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–µ —Å–ª–æ–≤–∞, –º–µ—Ç–∞—Ñ–æ—Ä—ã, –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            openness_markers = ["–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ", "–≤–æ–∑–º–æ–∂–Ω–æ", "–º–æ–∂–µ—Ç –±—ã—Ç—å", "—Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ", "–∏—Å–∫—É—Å—Å—Ç–≤–æ", "–Ω–µ–æ–±—ã—á–Ω–æ"]
            marker_count = sum(1 for marker in openness_markers if marker in answer_lower)
            return min(0.9, 0.3 + (marker_count * 0.1) + (answer_length / 1000))
        
        elif trait_name == "conscientiousness":
            # –ú–∞—Ä–∫–µ—Ä—ã –¥–æ–±—Ä–æ—Å–æ–≤–µ—Å—Ç–Ω–æ—Å—Ç–∏: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –ø–ª–∞–Ω—ã, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è
            conscient_markers = ["–ø–ª–∞–Ω", "–æ—Ä–≥–∞–Ω–∏–∑", "—Å–∏—Å—Ç–µ–º–Ω–æ", "–ø–æ—Ä—è–¥–æ–∫", "—Ä–µ–≥—É–ª—è—Ä–Ω–æ", "–¥–∏—Å—Ü–∏–ø–ª–∏–Ω"]
            marker_count = sum(1 for marker in conscient_markers if marker in answer_lower)
            return min(0.9, 0.3 + (marker_count * 0.15))
        
        elif trait_name == "extraversion":
            # –ú–∞—Ä–∫–µ—Ä—ã —ç–∫—Å—Ç—Ä–∞–≤–µ—Ä—Å–∏–∏: –ª—é–¥–∏, –æ–±—â–µ–Ω–∏–µ, —ç–Ω–µ—Ä–≥–∏—è
            extra_markers = ["–ª—é–¥–∏", "–¥—Ä—É–∑—å—è", "–æ–±—â–µ–Ω–∏–µ", "–∫–æ–º–∞–Ω–¥–∞", "—ç–Ω–µ—Ä–≥–∏—è", "–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å"]
            marker_count = sum(1 for marker in extra_markers if marker in answer_lower)
            return min(0.9, 0.2 + (marker_count * 0.12))
        
        elif trait_name == "agreeableness":
            # –ú–∞—Ä–∫–µ—Ä—ã –¥–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: –ø–æ–º–æ—â—å, –ø–æ–Ω–∏–º–∞–Ω–∏–µ, –≥–∞—Ä–º–æ–Ω–∏—è
            agree_markers = ["–ø–æ–º–æ—â—å", "–ø–æ–Ω–∏–º–∞–Ω–∏–µ", "–ø–æ–¥–¥–µ—Ä–∂–∫–∞", "–≥–∞—Ä–º–æ–Ω–∏—è", "–º–∏—Ä", "–∑–∞–±–æ—Ç–∞"]
            marker_count = sum(1 for marker in agree_markers if marker in answer_lower)
            return min(0.9, 0.4 + (marker_count * 0.1))
        
        elif trait_name == "neuroticism":
            # –ú–∞—Ä–∫–µ—Ä—ã –Ω–µ–π—Ä–æ—Ç–∏–∑–º–∞: —Ç—Ä–µ–≤–æ–≥–∞, —Å—Ç—Ä–µ—Å—Å, –ø—Ä–æ–±–ª–µ–º—ã
            neurotic_markers = ["–≤–æ–ª–Ω—É—é—Å—å", "—Ç—Ä–µ–≤–æ–≥–∞", "—Å—Ç—Ä–µ—Å—Å", "–ø—Ä–æ–±–ª–µ–º–∞", "—Å–ª–æ–∂–Ω–æ", "–±–æ—é—Å—å"]
            marker_count = sum(1 for marker in neurotic_markers if marker in answer_lower)
            return min(0.9, 0.1 + (marker_count * 0.15))
        
        # Fallback –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —á–µ—Ä—Ç
        return 0.5
    
    def _estimate_dynamic_trait(self, trait_name: str, ai_analysis: Dict, question_metadata: Dict) -> float:
        """–≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —á–µ—Ä—Ç—ã"""
        
        insights = ai_analysis.get("insights", {})
        emotional_state = ai_analysis.get("emotional_state", {})
        
        if trait_name == "resilience":
            # –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Å—Ç—Ä–µ—Å—Å—É
            valence = emotional_state.get("valence", 0.0)
            return max(0.1, min(0.9, 0.5 + valence * 0.3))
        
        elif trait_name == "authenticity":
            # –£—Ä–æ–≤–µ–Ω—å –∞—É—Ç–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç–∏ –∏–∑ AI –∞–Ω–∞–ª–∏–∑–∞
            return ai_analysis.get("meta_assessment", {}).get("answer_authenticity", 0.5)
        
        elif trait_name == "growth_mindset":
            # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —Ä–æ—Å—Ç
            growth_indicators = ["—Ä–∞–∑–≤–∏—Ç–∏–µ", "—Ä–æ—Å—Ç", "–∏–∑–º–µ–Ω–µ–Ω–∏–µ", "—É—á—É—Å—å", "—Ä–∞–∑–≤–∏–≤–∞—é—Å—å"]
            main_insight = insights.get("main", "").lower()
            indicator_count = sum(1 for indicator in growth_indicators if indicator in main_insight)
            return min(0.9, 0.4 + indicator_count * 0.15)
        
        elif trait_name == "emotional_granularity":
            # –¢–æ–Ω–∫–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è —ç–º–æ—Ü–∏–π  
            nuances = emotional_state.get("nuances", [])
            return min(0.9, 0.3 + len(nuances) * 0.1)
        
        elif trait_name == "cognitive_flexibility":
            # –ì–∏–±–∫–æ—Å—Ç—å –º—ã—à–ª–µ–Ω–∏—è
            complexity = question_metadata.get("psychology", {}).get("complexity", 1)
            return min(0.9, 0.2 + (complexity / 5.0) * 0.6)
        
        # Fallback
        return 0.5
    
    def _estimate_adaptive_trait(self, trait_name: str, ai_analysis: Dict, user_context: Dict) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö —á–µ—Ä—Ç (—Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ)"""
        
        if trait_name == "current_energy":
            # –¢–µ–∫—É—â–∏–π —É—Ä–æ–≤–µ–Ω—å —ç–Ω–µ—Ä–≥–∏–∏
            arousal = ai_analysis.get("emotional_state", {}).get("arousal", 0.5)
            time_factor = self._get_time_energy_factor(user_context.get("time_of_day", 12))
            return max(0.1, min(0.9, arousal * time_factor))
        
        elif trait_name == "stress_level":
            # –£—Ä–æ–≤–µ–Ω—å —Å—Ç—Ä–µ—Å—Å–∞
            return ai_analysis.get("meta_assessment", {}).get("fatigue_indicators", 0.3)
        
        elif trait_name == "openness_state":
            # –¢–µ–∫—É—â–∞—è –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –æ—Ç–∫—Ä—ã–≤–∞—Ç—å—Å—è
            depth_readiness = ai_analysis.get("meta_assessment", {}).get("insight_readiness", 0.5)
            trust_level = user_context.get("trust_level", 0.5)
            return (depth_readiness + trust_level) / 2.0
        
        elif trait_name == "creative_flow":
            # –í –ø–æ—Ç–æ–∫–µ –ª–∏ —Å–µ–π—á–∞—Å
            answer_length = user_context.get("answer_length", 50)
            response_time = user_context.get("response_time_seconds", 30)
            
            # –î–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç + –±—ã—Å—Ç—Ä–∞—è —Ä–µ–∞–∫—Ü–∏—è = –ø–æ—Ç–æ–∫
            if answer_length > 100 and response_time < 60:
                return 0.8
            elif answer_length < 20:
                return 0.2
            else:
                return 0.5
        
        elif trait_name == "social_battery":
            # –°–æ—Ü–∏–∞–ª—å–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è
            extraversion_context = user_context.get("current_extraversion", 0.5)
            session_length = user_context.get("session_length_minutes", 5)
            
            # –£–º–µ–Ω—å—à–∞–µ—Ç—Å—è –æ—Ç –≤—Ä–µ–º–µ–Ω–∏ —Å–µ—Å—Å–∏–∏ –¥–ª—è –∏–Ω—Ç—Ä–æ–≤–µ—Ä—Ç–æ–≤
            if extraversion_context < 0.5:  # –ò–Ω—Ç—Ä–æ–≤–µ—Ä—Ç
                battery = max(0.1, 0.9 - (session_length / 60.0) * 0.5)
            else:  # –≠–∫—Å—Ç—Ä–∞–≤–µ—Ä—Ç
                battery = max(0.3, 0.9 - (session_length / 120.0) * 0.3)
            
            return battery
        
        return 0.5
    
    def _extract_domain_specific_traits(self, ai_analysis: Dict, question_metadata: Dict) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–º–µ–Ω–Ω—ã—Ö —á–µ—Ä—Ç"""
        
        domain = question_metadata["classification"]["domain"]
        relevant_traits = self.config.DOMAIN_SPECIFIC_TRAITS.get(domain, [])
        
        if not relevant_traits:
            return {}
        
        domain_traits = {}
        ai_domain_data = ai_analysis.get("traits", {}).get("domain_specific", {}).get(domain, {})
        
        for trait in relevant_traits:
            if trait in ai_domain_data:
                domain_traits[trait] = round(float(ai_domain_data[trait]), 2)
            else:
                # –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞ –∏ —á–µ—Ä—Ç—ã
                estimated = self._estimate_domain_trait(trait, domain, ai_analysis)
                domain_traits[trait] = round(estimated, 2)
        
        return {domain: domain_traits} if domain_traits else {}
    
    def _estimate_domain_trait(self, trait_name: str, domain: str, ai_analysis: Dict) -> float:
        """–≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–æ–º–µ–Ω–Ω–æ–π —á–µ—Ä—Ç—ã"""
        
        insights = ai_analysis.get("insights", {}).get("main", "").lower()
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–º–µ–Ω–∞
        if domain == "RELATIONSHIPS":
            if trait_name == "empathy":
                empathy_markers = ["–ø–æ–Ω–∏–º–∞—é", "—á—É–≤—Å—Ç–≤—É—é", "—Å–æ–ø–µ—Ä–µ–∂–∏–≤–∞—é", "—ç–º–ø–∞—Ç–∏—è"]
                return min(0.9, 0.3 + sum(0.15 for marker in empathy_markers if marker in insights))
            elif trait_name == "boundaries":
                boundary_markers = ["–≥—Ä–∞–Ω–∏—Ü—ã", "—Å–∫–∞–∑–∞—Ç—å –Ω–µ—Ç", "—Å–≤–æ—ë –º–Ω–µ–Ω–∏–µ", "–æ—Ç–∫–∞–∑–∞—Ç—å"]
                return min(0.9, 0.2 + sum(0.2 for marker in boundary_markers if marker in insights))
                
        elif domain == "CAREER":
            if trait_name == "ambition":
                ambition_markers = ["–∫–∞—Ä—å–µ—Ä–∞", "—Ü–µ–ª–∏", "–¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è", "—É—Å–ø–µ—Ö", "–∞–º–±–∏—Ü–∏–∏"]
                return min(0.9, 0.3 + sum(0.12 for marker in ambition_markers if marker in insights))
            elif trait_name == "leadership":
                leader_markers = ["–ª–∏–¥–µ—Ä", "–∫–æ–º–∞–Ω–¥–∞", "—Ä—É–∫–æ–≤–æ–¥–∏—Ç—å", "–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å"]
                return min(0.9, 0.2 + sum(0.15 for marker in leader_markers if marker in insights))
        
        elif domain == "CREATIVITY":
            if trait_name == "divergent_thinking":
                creative_markers = ["–Ω–µ–æ–±—ã—á–Ω–æ", "–ø–æ-–¥—Ä—É–≥–æ–º—É", "–∫—Ä–µ–∞—Ç–∏–≤–Ω–æ", "–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞"]
                return min(0.9, 0.4 + sum(0.1 for marker in creative_markers if marker in insights))
        
        # Fallback - —Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞
        return 0.5
    
    def _generate_assessment_metadata(
        self,
        ai_analysis: Dict[str, Any],
        question_metadata: Dict[str, Any], 
        user_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞"""
        
        return {
            "confidence_scores": {
                trait: self._calculate_trait_confidence(trait, ai_analysis, question_metadata)
                for trait in self.config.BIG_FIVE_TRAITS
            },
            
            "data_quality": {
                "answer_length": user_context.get("answer_length", 0),
                "response_time": user_context.get("response_time_seconds", 0),
                "ai_model_confidence": ai_analysis.get("meta_assessment", {}).get("confidence", 0.5),
                "question_insight_potential": question_metadata.get("psychology", {}).get("insight_potential", 1)
            },
            
            "normalization_applied": {
                "contextual_adjustment": True,
                "depth_level_factor": question_metadata["classification"]["depth_level"],
                "time_of_day_factor": user_context.get("time_of_day"),
                "domain_specificity": question_metadata["classification"]["domain"]
            },
            
            "reliability_indicators": {
                "overall_reliability": self._calculate_overall_reliability(ai_analysis, user_context),
                "trait_variance_expected": self._calculate_expected_variance(question_metadata),
                "needs_validation": self._needs_additional_validation(ai_analysis, user_context)
            }
        }
    
    def _normalize_and_validate_traits(self, traits_dict: Dict[str, Dict]) -> Dict[str, Dict]:
        """–§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö —á–µ—Ä—Ç"""
        
        normalized = {}
        
        for trait_category, traits in traits_dict.items():
            normalized[trait_category] = {}
            
            for trait_name, value in traits.items():
                # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-1
                if isinstance(value, dict):
                    # –î–ª—è –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä (domain_specific)
                    normalized[trait_category][trait_name] = {
                        k: max(0.0, min(1.0, round(float(v), 2))) 
                        for k, v in value.items()
                        if isinstance(v, (int, float))
                    }
                else:
                    # –î–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                    normalized[trait_category][trait_name] = max(0.0, min(1.0, round(float(value), 2)))
        
        return normalized
    
    def _get_fallback_traits(self, question_metadata: Dict, user_context: Dict) -> Dict[str, Any]:
        """Fallback —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —á–µ—Ä—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è"""
        
        logger.warning("üîÑ Using fallback trait extraction")

        # –ë–∞–∑–æ–≤—ã–µ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        timestamp = datetime.now().isoformat()
        big_five_fallback = {trait: 0.5 for trait in self.config.BIG_FIVE_TRAITS}

        fallback = {
            "version": "2.0",
            "timestamp": timestamp,  # –î–ª—è DB constraint
            "extracted_at": timestamp,
            "big_five": big_five_fallback,  # –î–ª—è DB constraint (–ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø)
            "traits": {
                "big_five": big_five_fallback,
                "dynamic_traits": {trait: 0.5 for trait in self.config.DYNAMIC_TRAITS},
                "adaptive_traits": {trait: 0.5 for trait in self.config.ADAPTIVE_TRAITS},
                "domain_specific": {}
            },
            "assessment_metadata": {
                "is_fallback": True,
                "reliability_indicators": {
                    "overall_reliability": 0.1,  # –ù–∏–∑–∫–∞—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å
                    "needs_validation": True
                }
            },
            "extraction_context": {
                "question_domain": question_metadata.get("classification", {}).get("domain", "UNKNOWN"),
                "fallback_reason": "extraction_error"
            }
        }
        
        return fallback
    
    def _calculate_trait_confidence(self, trait_name: str, ai_analysis: Dict, question_metadata: Dict) -> float:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ—Ü–µ–Ω–∫–µ —á–µ—Ä—Ç—ã"""
        
        # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –æ—Ç AI
        base_confidence = ai_analysis.get("meta_assessment", {}).get("confidence", 0.5)
        
        # –ú–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        insight_potential = question_metadata.get("psychology", {}).get("insight_potential", 1) / 5.0
        complexity = question_metadata.get("psychology", {}).get("complexity", 1) / 5.0
        
        # –ß–µ–º –≤—ã—à–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∏–Ω—Å–∞–π—Ç–∞ –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å, —Ç–µ–º –±–æ–ª—å—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        final_confidence = min(0.95, base_confidence * (1 + insight_potential * 0.2 + complexity * 0.1))
        
        return round(final_confidence, 2)
    
    def _calculate_overall_reliability(self, ai_analysis: Dict, user_context: Dict) -> float:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –æ–±—â—É—é –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–∫–∏"""
        
        factors = []
        
        # –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
        answer_length = user_context.get("answer_length", 0)
        if answer_length > 100:
            factors.append(0.9)
        elif answer_length > 50:
            factors.append(0.7)
        else:
            factors.append(0.3)
        
        # –í—Ä–µ–º—è –æ–±–¥—É–º—ã–≤–∞–Ω–∏—è
        response_time = user_context.get("response_time_seconds", 30)
        if 20 <= response_time <= 180:  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–±–¥—É–º—ã–≤–∞–Ω–∏—è
            factors.append(0.8)
        else:
            factors.append(0.6)
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ AI
        if all(key in ai_analysis for key in ["emotional_state", "traits", "insights"]):
            factors.append(0.9)
        else:
            factors.append(0.5)
        
        # –°—Ä–µ–¥–Ω–µ–µ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ
        return round(sum(factors) / len(factors), 2)

    def _get_time_energy_factor(self, hour: int) -> float:
        """
        –û—Ü–µ–Ω–∫–∞ —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ñ–∞–∫—Ç–æ—Ä–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤—Ä–µ–º–µ–Ω–∏ —Å—É—Ç–æ–∫

        Args:
            hour: –ß–∞—Å –¥–Ω—è (0-23)

        Returns:
            float: –ú–Ω–æ–∂–∏—Ç–µ–ª—å —ç–Ω–µ—Ä–≥–∏–∏ (0.5 - 1.2)
        """

        # –£—Ç—Ä–æ (6-9): –≤—ã—Å–æ–∫–∞—è —ç–Ω–µ—Ä–≥–∏—è
        if 6 <= hour < 9:
            return 1.2

        # –î–µ–Ω—å (9-12): –ø–∏–∫–æ–≤–∞—è —ç–Ω–µ—Ä–≥–∏—è
        elif 9 <= hour < 12:
            return 1.3

        # –ü–æ—Å–ª–µ –æ–±–µ–¥–∞ (12-14): —Å–ø–∞–¥
        elif 12 <= hour < 14:
            return 0.9

        # –ü–æ—Å–ª–µ–æ–±–µ–¥–µ–Ω–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (14-18): —Ö–æ—Ä–æ—à–∞—è —ç–Ω–µ—Ä–≥–∏—è
        elif 14 <= hour < 18:
            return 1.1

        # –í–µ—á–µ—Ä (18-22): —Å–Ω–∏–∂–µ–Ω–∏–µ
        elif 18 <= hour < 22:
            return 0.8

        # –ù–æ—á—å (22-6): –Ω–∏–∑–∫–∞—è —ç–Ω–µ—Ä–≥–∏—è
        else:
            return 0.5

    def _calculate_expected_variance(self, question_metadata: Dict) -> float:
        """
        –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –æ–∂–∏–¥–∞–µ–º—É—é –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç—Ä–µ–π—Ç–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –≤–æ–ø—Ä–æ—Å–∞

        Args:
            question_metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å–∞

        Returns:
            float –æ—Ç 0.0 –¥–æ 1.0 (0.0 = –Ω–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å, 1.0 = –≤—ã—Å–æ–∫–∞—è)
        """

        # –ë–∞–∑–æ–≤–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        variance = 0.3

        # –ì–ª—É–±–∏–Ω–∞ –≤–æ–ø—Ä–æ—Å–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        depth_level = question_metadata.get("classification", {}).get("depth_level", "SURFACE")
        depth_variance_map = {
            "SURFACE": 0.1,  # –ü–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã - –Ω–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
            "CONSCIOUS": 0.2,
            "EDGE": 0.4,  # –ü–æ–≥—Ä–∞–Ω–∏—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã - –≤—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
            "SHADOW": 0.5,
            "CORE": 0.6  # –ì–ª—É–±–∏–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        }
        variance = depth_variance_map.get(depth_level, 0.3)

        # –≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞ –≤–æ–ø—Ä–æ—Å–∞
        energy = question_metadata.get("classification", {}).get("energy_dynamic", "NEUTRAL")
        if energy in ["HEAVY", "PROCESSING"]:
            variance += 0.1  # –¢—è–∂–µ–ª—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–∞—é—Ç –±–æ–ª—å—à–µ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏

        # –î–æ–º–µ–Ω –º–æ–∂–µ—Ç –≤–ª–∏—è—Ç—å
        domain = question_metadata.get("classification", {}).get("domain", "")
        if domain in ["FEARS", "SHADOW", "PAST"]:
            variance += 0.1  # –°–ª–æ–∂–Ω—ã–µ –¥–æ–º–µ–Ω—ã –¥–∞—é—Ç –±–æ–ª—å—à–µ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏

        return min(1.0, max(0.0, variance))

    def _needs_additional_validation(self, ai_analysis: Dict, user_context: Dict) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω—É–∂–Ω–∞ –ª–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è"""

        # –ù—É–∂–Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—è –µ—Å–ª–∏:
        return any([
            self._calculate_overall_reliability(ai_analysis, user_context) < 0.6,
            user_context.get("answer_length", 0) < 20,
            user_context.get("question_number", 1) < 3,  # –ü–µ—Ä–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –º–µ–Ω–µ–µ –Ω–∞–¥–µ–∂–Ω—ã
            ai_analysis.get("meta_assessment", {}).get("fatigue_indicators", 0) > 0.7
        ])