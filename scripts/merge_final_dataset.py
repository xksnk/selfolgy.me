#!/usr/bin/env python3
"""
–§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤:
- 943 –≤–æ–ø—Ä–æ—Å–∞ –∏–∑ selfology_intelligent_core_enhanced.json
- 390 –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ 8 –Ω–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤ (05-09, 11-13)
= 1333 –≤–æ–ø—Ä–æ—Å–∞ (—Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)
"""

import json
from pathlib import Path
from collections import Counter

# –ü—É—Ç–∏
DATA_DIR = Path("/home/ksnk/n8n-enterprise/projects/selfology/intelligent_question_core/data")
BLOCKS_DIR = DATA_DIR / "generated_blocks"
ENHANCED_FILE = DATA_DIR / "selfology_intelligent_core_enhanced.json"
OUTPUT_FILE = DATA_DIR / "selfology_intelligent_core_complete.json"

def load_json(filepath):
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å JSON —Ñ–∞–π–ª"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_json(data, filepath):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å JSON —Ñ–∞–π–ª"""
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def check_duplicates(existing, new):
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ ID –∏ —Ç–µ–∫—Å—Ç—É"""
    existing_ids = {q['id'] for q in existing}
    existing_texts = {q['text'].lower().strip() for q in existing}

    duplicates = []
    unique_new = []

    for question in new:
        q_id = question['id']
        q_text = question['text'].lower().strip()

        if q_id in existing_ids or q_text in existing_texts:
            duplicates.append(question)
        else:
            unique_new.append(question)

    return unique_new, duplicates

def analyze_distribution(questions):
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤"""
    domains = Counter()
    depths = Counter()
    energies = Counter()
    stages = Counter()

    for q in questions:
        cls = q.get('classification', {})
        domains[cls.get('domain', 'UNKNOWN')] += 1
        depths[cls.get('depth_level', 'UNKNOWN')] += 1
        energies[cls.get('energy_dynamic', 'UNKNOWN')] += 1
        stages[cls.get('journey_stage', 'UNKNOWN')] += 1

    return {
        'domains': dict(domains),
        'depths': dict(depths),
        'energies': dict(energies),
        'stages': dict(stages)
    }

def main():
    print("üîÑ –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")
    print("=" * 60)

    # 1. –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ 943 –≤–æ–ø—Ä–æ—Å–∞
    print("\nüìñ –ó–∞–≥—Ä—É–∂–∞—é —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã...")
    enhanced_data = load_json(ENHANCED_FILE)

    # –§–∞–π–ª –∏–º–µ–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É {metadata: {...}, questions: [...]}
    if isinstance(enhanced_data, dict) and 'questions' in enhanced_data:
        existing_questions = enhanced_data['questions']
        print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(existing_questions)} –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ enhanced dataset")
    else:
        existing_questions = enhanced_data
        print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(existing_questions)} –≤–æ–ø—Ä–æ—Å–æ–≤")

    # 2. –ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—ã–µ –±–ª–æ–∫–∏
    print("\nüì¶ –ó–∞–≥—Ä—É–∂–∞—é –Ω–æ–≤—ã–µ –±–ª–æ–∫–∏...")
    new_blocks = [
        "05_EMOTIONS.json",
        "06_RELATIONSHIPS.json",
        "07_GOALS.json",
        "08_FEARS.json",
        "09_VALUES.json",
        "11_DEEPENING.json",
        "12_INTEGRATING.json",
        "13_TRANSFORMING.json"
    ]

    new_questions = []
    for block_file in new_blocks:
        block_path = BLOCKS_DIR / block_file
        if block_path.exists():
            block_data = load_json(block_path)

            # –ë–ª–æ–∫–∏ –∏–º–µ—é—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É {block_info: {...}, questions: [...]}
            if isinstance(block_data, dict) and 'questions' in block_data:
                questions = block_data['questions']
                new_questions.extend(questions)
                print(f"   ‚úÖ {block_file}: {len(questions)} –≤–æ–ø—Ä–æ—Å–æ–≤")
            else:
                new_questions.extend(block_data)
                print(f"   ‚úÖ {block_file}: {len(block_data)} –≤–æ–ø—Ä–æ—Å–æ–≤")
        else:
            print(f"   ‚ö†Ô∏è  {block_file}: –Ω–µ –Ω–∞–π–¥–µ–Ω")

    print(f"\n   üìä –í—Å–µ–≥–æ –Ω–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: {len(new_questions)}")

    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
    unique_new, duplicates = check_duplicates(existing_questions, new_questions)

    if duplicates:
        print(f"   ‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(duplicates)}")
        print("   üìù –ü–µ—Ä–≤—ã–µ 3 –¥—É–±–ª–∏–∫–∞—Ç–∞:")
        for dup in duplicates[:3]:
            print(f"      - {dup['id']}: {dup['text'][:60]}...")
    else:
        print(f"   ‚úÖ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    print(f"   ‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: {len(unique_new)}")

    # 4. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
    print("\nüîó –û–±—ä–µ–¥–∏–Ω—è—é –≤–æ–ø—Ä–æ—Å—ã...")
    all_questions = existing_questions + unique_new
    print(f"   ‚úÖ –ò—Ç–æ–≥–æ: {len(all_questions)} –≤–æ–ø—Ä–æ—Å–æ–≤")

    # 5. –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    print("\nüìä –ê–Ω–∞–ª–∏–∑ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:")
    distribution = analyze_distribution(all_questions)

    print("\n   –î–æ–º–µ–Ω—ã:")
    for domain, count in sorted(distribution['domains'].items(), key=lambda x: -x[1]):
        print(f"      {domain:20s}: {count:4d} –≤–æ–ø—Ä–æ—Å–æ–≤")

    print("\n   –ì–ª—É–±–∏–Ω–∞:")
    depth_order = ['SURFACE', 'CONSCIOUS', 'EDGE', 'SHADOW', 'CORE']
    for depth in depth_order:
        count = distribution['depths'].get(depth, 0)
        if count > 0:
            print(f"      {depth:20s}: {count:4d} –≤–æ–ø—Ä–æ—Å–æ–≤")

    print("\n   –≠–Ω–µ—Ä–≥–µ—Ç–∏–∫–∞:")
    for energy, count in sorted(distribution['energies'].items(), key=lambda x: -x[1]):
        print(f"      {energy:20s}: {count:4d} –≤–æ–ø—Ä–æ—Å–æ–≤")

    print("\n   –°—Ç–∞–¥–∏–∏ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è:")
    for stage, count in sorted(distribution['stages'].items(), key=lambda x: -x[1]):
        print(f"      {stage:20s}: {count:4d} –≤–æ–ø—Ä–æ—Å–æ–≤")

    # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç...")

    # –°–æ–∑–¥–∞—é —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    from datetime import datetime
    final_data = {
        "metadata": {
            "version": "3.0",
            "title": "Selfology Intelligent Question Core - Complete",
            "description": "–ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ —Å–∞–º–æ–ø–æ–∑–Ω–∞–Ω–∏—è",
            "total_questions": len(all_questions),
            "original_questions": len(existing_questions),
            "new_generated_questions": len(unique_new),
            "duplicates_removed": len(duplicates),
            "last_updated": datetime.now().isoformat(),
            "sources": [
                "selfology_intelligent_core_enhanced.json (943 questions)",
                "ai_generated_8_blocks_claude_opus_4 (390 questions)"
            ],
            "distribution": distribution
        },
        "questions": all_questions
    }

    save_json(final_data, OUTPUT_FILE)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∞–π–ª–∞
    file_size_mb = OUTPUT_FILE.stat().st_size / (1024 * 1024)
    print(f"   ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {OUTPUT_FILE}")
    print(f"   üì¶ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size_mb:.2f} MB")

    # 7. –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞
    print("\n" + "=" * 60)
    print("‚ú® –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–í–û–î–ö–ê")
    print("=" * 60)
    print(f"üìä –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã:     {len(existing_questions):4d}")
    print(f"‚ûï –ù–æ–≤—ã–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã: {len(unique_new):4d}")
    if duplicates:
        print(f"‚ûñ –î—É–±–ª–∏–∫–∞—Ç—ã (–ø—Ä–æ–ø—É—â–µ–Ω–æ):    {len(duplicates):4d}")
    print(f"üéØ –ò–¢–û–ì–û –≤–æ–ø—Ä–æ—Å–æ–≤:           {len(all_questions):4d}")
    print("=" * 60)

    # 8. –°–æ–∑–¥–∞–Ω–∏–µ summary —Ñ–∞–π–ª–∞
    summary = {
        "total_questions": len(all_questions),
        "sources": {
            "existing_enhanced": len(existing_questions),
            "new_blocks": len(unique_new),
            "duplicates_removed": len(duplicates)
        },
        "distribution": distribution,
        "file": str(OUTPUT_FILE),
        "size_mb": round(file_size_mb, 2)
    }

    summary_file = DATA_DIR / "FINAL_MERGE_SUMMARY.json"
    save_json(summary, summary_file)
    print(f"\nüìã –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {summary_file}")

    print("\n‚úÖ –ì–æ—Ç–æ–≤–æ! –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω.")

if __name__ == "__main__":
    main()
