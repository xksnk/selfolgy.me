#!/usr/bin/env python3
"""
–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ –±–∞–∑–µ
–≠—Ç–∞–ø 1: –¢–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
–≠—Ç–∞–ø 2: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ (—á–µ—Ä–µ–∑ embeddings)
"""
import json
from pathlib import Path
import re
from collections import defaultdict
import os
from openai import OpenAI
import numpy as np
from tqdm import tqdm
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env
load_dotenv()

def normalize_text(text):
    """–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    # –£–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    text = re.sub(r'\s+', ' ', text)
    text = text.strip().lower()
    # –£–±—Ä–∞—Ç—å –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –≤ –∫–æ–Ω—Ü–µ
    text = text.rstrip('?.!,;:')
    return text

def cosine_similarity(vec1, vec2):
    """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

def main():
    print("üîç –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø –í–û–ü–†–û–°–û–í\n")
    print("="*80)

    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É
    data_file = Path('intelligent_question_core/data/selfology_questions_with_generated.json')
    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    questions = data['questions']
    print(f"üìä –í—Å–µ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(questions)}\n")

    # ========================================
    # –≠–¢–ê–ü 1: –¢–û–ß–ù–´–ï –î–£–ë–õ–ò–ö–ê–¢–´
    # ========================================
    print("="*80)
    print("üìã –≠–¢–ê–ü 1: –ü–û–ò–°–ö –¢–û–ß–ù–´–• –î–£–ë–õ–ò–ö–ê–¢–û–í\n")

    # –ò–Ω–¥–µ–∫—Å: –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç -> —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤
    text_index = defaultdict(list)

    for q in questions:
        norm_text = normalize_text(q['text'])
        text_index[norm_text].append(q)

    # –ù–∞–π—Ç–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã
    exact_duplicates = 0
    duplicate_groups = []

    for norm_text, q_list in text_index.items():
        if len(q_list) > 1:
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ ID (–ø–µ—Ä–≤—ã–π –±—É–¥–µ—Ç –º–∞—Å—Ç–µ—Ä)
            q_list.sort(key=lambda x: x['id'])
            master = q_list[0]
            duplicates = q_list[1:]

            duplicate_groups.append({
                'master': master['id'],
                'duplicates': [q['id'] for q in duplicates],
                'text': master['text'][:80]
            })

            # –ü–æ–º–µ—Ç–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
            for dup in duplicates:
                dup['duplicate_of'] = master['id']
                exact_duplicates += 1

    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(duplicate_groups)}")
    print(f"‚úÖ –í—Å–µ–≥–æ —Ç–æ—á–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {exact_duplicates}")

    if duplicate_groups:
        print(f"\nüîç –ü–†–ò–ú–ï–†–´ –î–£–ë–õ–ò–ö–ê–¢–û–í:\n")
        for i, group in enumerate(duplicate_groups[:5], 1):
            print(f"{i}. –ú–∞—Å—Ç–µ—Ä: {group['master']}")
            print(f"   –¢–µ–∫—Å—Ç: {group['text']}...")
            print(f"   –î—É–±–ª–∏–∫–∞—Ç—ã: {', '.join(group['duplicates'])}\n")

    # ========================================
    # –≠–¢–ê–ü 2: –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò –ü–û–•–û–ñ–ò–ï
    # ========================================
    print("\n" + "="*80)
    print("üß† –≠–¢–ê–ü 2: –ü–û–ò–°–ö –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò –ü–û–•–û–ñ–ò–• –í–û–ü–†–û–°–û–í\n")

    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å API –∫–ª—é—á
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("‚ö†Ô∏è  OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
        print("   –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑")
        semantic_similar = 0
    else:
        client = OpenAI(api_key=api_key)

        # –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –º–∞—Å—Ç–µ—Ä-–≤–æ–ø—Ä–æ—Å—ã (–±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)
        master_questions = [q for q in questions if 'duplicate_of' not in q]
        print(f"üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(master_questions)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")

        # –°–æ–∑–¥–∞—Ç—å embeddings –¥–ª—è –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        print("\nüîÑ –°–æ–∑–¥–∞–Ω–∏–µ embeddings...")
        texts = [q['text'] for q in master_questions]

        embeddings = []
        batch_size = 100

        for i in tqdm(range(0, len(texts), batch_size), desc="Embeddings"):
            batch = texts[i:i+batch_size]
            response = client.embeddings.create(
                model="text-embedding-3-small",
                input=batch
            )
            batch_embeddings = [item.embedding for item in response.data]
            embeddings.extend(batch_embeddings)

        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(embeddings)} embeddings\n")

        # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö
        print("üîç –ü–æ–∏—Å–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö (similarity > 0.85)...\n")

        semantic_similar = 0
        similar_pairs = []

        for i in tqdm(range(len(master_questions)), desc="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ"):
            q1 = master_questions[i]
            emb1 = embeddings[i]

            similar_to = []

            for j in range(i+1, len(master_questions)):
                q2 = master_questions[j]
                emb2 = embeddings[j]

                similarity = cosine_similarity(emb1, emb2)

                if similarity > 0.85:
                    similar_to.append({
                        'id': q2['id'],
                        'similarity': round(float(similarity), 3),
                        'text': q2['text'][:60]
                    })
                    semantic_similar += 1

                    similar_pairs.append({
                        'q1_id': q1['id'],
                        'q1_text': q1['text'][:60],
                        'q2_id': q2['id'],
                        'q2_text': q2['text'][:60],
                        'similarity': round(float(similarity), 3)
                    })

            if similar_to:
                if 'similar_to' not in q1:
                    q1['similar_to'] = []
                q1['similar_to'].extend(similar_to)

        print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä: {semantic_similar}")

        if similar_pairs:
            print(f"\nüîç –ü–†–ò–ú–ï–†–´ –ü–û–•–û–ñ–ò–• –í–û–ü–†–û–°–û–í:\n")
            for i, pair in enumerate(similar_pairs[:10], 1):
                print(f"{i}. [{pair['similarity']}] –ü–æ—Ö–æ–∂–µ—Å—Ç—å:")
                print(f"   {pair['q1_id']}: {pair['q1_text']}...")
                print(f"   {pair['q2_id']}: {pair['q2_text']}...\n")

    # ========================================
    # –°–û–•–†–ê–ù–ï–ù–ò–ï
    # ========================================
    print("="*80)
    print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\n")

    output_file = Path('intelligent_question_core/data/selfology_questions_deduplicated.json')

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_file}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    duplicates_count = sum(1 for q in questions if 'duplicate_of' in q)
    similar_count = sum(1 for q in questions if 'similar_to' in q)
    clean_count = len(questions) - duplicates_count

    print(f"\n\n{'='*80}")
    print(f"üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n")
    print(f"–í—Å–µ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(questions)}")
    print(f"–¢–æ—á–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicates_count}")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: {clean_count}")
    print(f"–í–æ–ø—Ä–æ—Å–æ–≤ —Å –ø–æ—Ö–æ–∂–∏–º–∏: {similar_count}")
    print(f"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä: {semantic_similar}")

    print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    print(f"‚Ä¢ –ü—Ä–∏ —Å–µ–∫–≤–µ–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã —Å duplicate_of")
    print(f"‚Ä¢ –í–æ–ø—Ä–æ—Å—ã —Å similar_to –≤—ã–±–∏—Ä–∞—Ç—å –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ - –∏–Ω–æ–≥–¥–∞ –Ω—É–∂–Ω—ã –≤–∞—Ä–∏–∞—Ü–∏–∏")
    print(f"‚Ä¢ –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –º–æ–∂–Ω–æ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ —É–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã")

if __name__ == '__main__':
    main()
