#!/usr/bin/env python3
"""
–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö 693 –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ 250 –Ω–æ–≤—ã–º–∏
"""

import json
from pathlib import Path
from collections import Counter
from datetime import datetime

# –ü—É—Ç–∏
BASE_DIR = Path("/home/ksnk/n8n-enterprise/projects/selfology")
EXISTING_FILE = BASE_DIR / "intelligent_question_core/data/selfology_intelligent_core.json"
GENERATED_DIR = BASE_DIR / "intelligent_question_core/data/generated_blocks"
OUTPUT_FILE = BASE_DIR / "intelligent_question_core/data/selfology_intelligent_core_enhanced.json"

def load_existing_questions():
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ 693 –≤–æ–ø—Ä–æ—Å–∞"""
    print(f"üìÇ –ß–∏—Ç–∞—é —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã: {EXISTING_FILE}")

    with open(EXISTING_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)

    questions = data.get('questions', [])
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(questions)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")

    return data, questions

def load_generated_blocks():
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –±–ª–æ–∫–∏"""
    print(f"\nüìÇ –ß–∏—Ç–∞—é –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –±–ª–æ–∫–∏: {GENERATED_DIR}")

    all_questions = []
    block_files = sorted(GENERATED_DIR.glob("*.json"))

    for block_file in block_files:
        if block_file.name in ['README.md', 'SUMMARY.md']:
            continue

        with open(block_file, 'r', encoding='utf-8') as f:
            questions = json.load(f)

        all_questions.extend(questions)
        print(f"   ‚úÖ {block_file.name}: {len(questions)} –≤–æ–ø—Ä–æ—Å–æ–≤")

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_questions)} –Ω–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")

    return all_questions

def check_duplicates(existing, new):
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ ID –∏ —Ç–µ–∫—Å—Ç—É"""
    existing_ids = {q['id'] for q in existing}
    existing_texts = {q['text'].lower().strip() for q in existing}

    duplicates_by_id = []
    duplicates_by_text = []
    unique_new = []

    for q in new:
        q_id = q.get('id', '')
        q_text = q.get('text', '').lower().strip()

        if q_id in existing_ids:
            duplicates_by_id.append(q_id)
        elif q_text in existing_texts:
            duplicates_by_text.append(q_text[:50] + "...")
        else:
            unique_new.append(q)
            existing_ids.add(q_id)
            existing_texts.add(q_text)

    if duplicates_by_id:
        print(f"\n‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {len(duplicates_by_id)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ ID")

    if duplicates_by_text:
        print(f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {len(duplicates_by_text)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç—É")

    return unique_new

def analyze_distribution(questions):
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤"""
    domains = Counter()
    depth_levels = Counter()
    energy_dynamics = Counter()
    journey_stages = Counter()

    for q in questions:
        classification = q.get('classification', {})
        domains[classification.get('domain', 'UNKNOWN')] += 1
        depth_levels[classification.get('depth_level', 'UNKNOWN')] += 1
        energy_dynamics[classification.get('energy_dynamic', 'UNKNOWN')] += 1
        journey_stages[classification.get('journey_stage', 'UNKNOWN')] += 1

    return {
        'domains': dict(domains),
        'depth_levels': dict(depth_levels),
        'energy_dynamics': dict(energy_dynamics),
        'journey_stages': dict(journey_stages)
    }

def main():
    print("üîÑ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤ Selfology\n")
    print("=" * 60)

    # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
    existing_data, existing_questions = load_existing_questions()

    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—ã–µ
    new_questions = load_generated_blocks()

    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
    print(f"\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
    unique_new = check_duplicates(existing_questions, new_questions)

    print(f"\n‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: {len(unique_new)}")

    # –û–±—ä–µ–¥–∏–Ω–∏—Ç—å
    all_questions = existing_questions + unique_new
    total = len(all_questions)

    print(f"\nüìä –ò—Ç–æ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {total}")
    print(f"   - –°—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö: {len(existing_questions)}")
    print(f"   - –î–æ–±–∞–≤–ª–µ–Ω–æ –Ω–æ–≤—ã—Ö: {len(unique_new)}")

    # –ê–Ω–∞–ª–∏–∑
    print(f"\nüìà –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è...")
    distribution = analyze_distribution(all_questions)

    print(f"\nüè∑Ô∏è  –ü–æ –¥–æ–º–µ–Ω–∞–º:")
    for domain, count in sorted(distribution['domains'].items(), key=lambda x: -x[1]):
        print(f"   {domain:20} - {count:3} –≤–æ–ø—Ä–æ—Å–æ–≤")

    print(f"\nüìä –ü–æ –≥–ª—É–±–∏–Ω–µ:")
    for level, count in sorted(distribution['depth_levels'].items(), key=lambda x: -x[1]):
        print(f"   {level:20} - {count:3} –≤–æ–ø—Ä–æ—Å–æ–≤")

    # –°–æ–∑–¥–∞—Ç—å –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    output_data = {
        "metadata": {
            "version": "2.1",
            "title": "Selfology Intelligent Question Core - Enhanced",
            "description": "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ —Å–∞–º–æ–ø–æ–∑–Ω–∞–Ω–∏—è",
            "total_questions": total,
            "original_questions": len(existing_questions),
            "generated_questions": len(unique_new),
            "last_updated": datetime.now().isoformat(),
            "sources": [
                "original_693_professional_questions",
                "ai_generated_250_questions_claude_opus_4"
            ],
            "distribution": distribution
        },
        "questions": all_questions
    }

    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞...")
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {OUTPUT_FILE}")
    print(f"üì¶ –†–∞–∑–º–µ—Ä: {OUTPUT_FILE.stat().st_size / 1024:.1f} KB")

    # –¢–∞–∫–∂–µ –æ–±–Ω–æ–≤–∏—Ç—å –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª (–±—ç–∫–∞–ø —Å–æ–∑–¥–∞–Ω)
    backup_file = EXISTING_FILE.parent / f"selfology_intelligent_core.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

    print(f"\nüîÑ –°–æ–∑–¥–∞–Ω–∏–µ –±—ç–∫–∞–ø–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞...")
    with open(EXISTING_FILE, 'r', encoding='utf-8') as f_in:
        with open(backup_file, 'w', encoding='utf-8') as f_out:
            f_out.write(f_in.read())

    print(f"‚úÖ –ë—ç–∫–∞–ø: {backup_file.name}")

    print(f"\n‚ú® –ì–æ—Ç–æ–≤–æ!")
    print(f"\nüìÅ –§–∞–π–ª—ã:")
    print(f"   - –û—Å–Ω–æ–≤–Ω–æ–π (enhanced): {OUTPUT_FILE.name}")
    print(f"   - –û—Ä–∏–≥–∏–Ω–∞–ª (693): {EXISTING_FILE.name}")
    print(f"   - –ë—ç–∫–∞–ø: {backup_file.name}")

if __name__ == "__main__":
    main()
