#!/usr/bin/env python3
"""
–ú–∏–≥—Ä–∞—Ü–∏—è –Ω–∞ –Ω–æ–≤—ã–π —Ñ–∞–π–ª –≤–æ–ø—Ä–æ—Å–æ–≤ (1513 –≤–æ–ø—Ä–æ—Å–æ–≤).
–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç search_indexes –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.
"""
import json
from pathlib import Path
from collections import defaultdict
from datetime import datetime

DATA_DIR = Path(__file__).parent.parent / "intelligent_question_core" / "data"
OLD_FILE = DATA_DIR / "selfology_intelligent_core.json"
NEW_FILE = DATA_DIR / "selfology_final_sequenced.json"
OUTPUT_FILE = DATA_DIR / "selfology_core_v2.json"


def build_search_indexes(questions: list) -> dict:
    """–°—Ç—Ä–æ–∏—Ç –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤."""
    indexes = {
        "metadata": {
            "total_questions": len(questions),
            "generated_at": datetime.now().isoformat(),
            "version": "2.0"
        },
        "by_classification": {
            "domain": defaultdict(list),
            "depth_level": defaultdict(list),
            "energy_dynamic": defaultdict(list),
            "journey_stage": defaultdict(list)
        },
        "by_psychology": {
            "safety_level": defaultdict(list),
            "complexity": defaultdict(list),
            "emotional_weight": defaultdict(list),
            "trust_requirement": defaultdict(list)
        },
        "combinations": {
            "domain_depth": defaultdict(list),
            "domain_energy": defaultdict(list)
        },
        "by_source": defaultdict(list),
        "keywords": defaultdict(list)
    }

    for q in questions:
        qid = q.get("id")
        if not qid:
            continue

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        classification = q.get("classification", {})
        for field in ["domain", "depth_level", "energy_dynamic", "journey_stage"]:
            value = classification.get(field)
            if value:
                indexes["by_classification"][field][value].append(qid)

        # –ü—Å–∏—Ö–æ–ª–æ–≥–∏—è
        psychology = q.get("psychology", {})
        for field in ["safety_level", "complexity", "emotional_weight", "trust_requirement"]:
            value = psychology.get(field)
            if value is not None:
                indexes["by_psychology"][field][str(value)].append(qid)

        # –ö–æ–º–±–∏–Ω–∞—Ü–∏–∏ (–¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —á–∞—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)
        domain = classification.get("domain")
        depth = classification.get("depth_level")
        energy = classification.get("energy_dynamic")

        if domain and depth:
            key = f"{domain}_{depth}"
            indexes["combinations"]["domain_depth"][key].append(qid)
        if domain and energy:
            key = f"{domain}_{energy}"
            indexes["combinations"]["domain_energy"][key].append(qid)

        # –ò—Å—Ç–æ—á–Ω–∏–∫
        source = q.get("source", "unknown")
        indexes["by_source"][source].append(qid)

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º defaultdict –≤ –æ–±—ã—á–Ω—ã–µ dict
    def convert_defaultdict(obj):
        if isinstance(obj, defaultdict):
            return {k: convert_defaultdict(v) for k, v in obj.items()}
        elif isinstance(obj, dict):
            return {k: convert_defaultdict(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return obj
        return obj

    return convert_defaultdict(indexes)


def migrate():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –º–∏–≥—Ä–∞—Ü–∏–∏."""
    print("üîÑ –ó–∞–≥—Ä—É–∂–∞—é —Ñ–∞–π–ª—ã...")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª (–¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä)
    with open(OLD_FILE, 'r', encoding='utf-8') as f:
        old_data = json.load(f)
    print(f"  ‚úÖ –°—Ç–∞—Ä—ã–π —Ñ–∞–π–ª: {len(old_data.get('questions', []))} –≤–æ–ø—Ä–æ—Å–æ–≤")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª
    with open(NEW_FILE, 'r', encoding='utf-8') as f:
        new_data = json.load(f)
    print(f"  ‚úÖ –ù–æ–≤—ã–π —Ñ–∞–π–ª: {len(new_data.get('questions', []))} –≤–æ–ø—Ä–æ—Å–æ–≤")

    # –°—Ç—Ä–æ–∏–º –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    print("\nüîß –°—Ç—Ä–æ—é search_indexes...")
    questions = new_data.get("questions", [])
    search_indexes = build_search_indexes(questions)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–æ–º–µ–Ω–∞–º
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–æ–º–µ–Ω–∞–º:")
    for domain, ids in search_indexes["by_classification"]["domain"].items():
        print(f"  {domain}: {len(ids)} –≤–æ–ø—Ä–æ—Å–æ–≤")

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª
    output = {
        "core_metadata": {
            "version": "2.0",
            "generated_at": datetime.now().isoformat(),
            "total_questions": len(questions),
            "source": "selfology_final_sequenced.json",
            "migration_from": "selfology_intelligent_core.json"
        },
        "questions": questions,
        "connections": old_data.get("connections", []),  # –ö–æ–ø–∏—Ä—É–µ–º —Å–≤—è–∑–∏
        "taxonomy": old_data.get("taxonomy", {}),        # –ö–æ–ø–∏—Ä—É–µ–º —Ç–∞–∫—Å–æ–Ω–æ–º–∏—é
        "energy_rules": old_data.get("energy_rules", {}), # –ö–æ–ø–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª–∞ —ç–Ω–µ—Ä–≥–∏–∏
        "search_indexes": search_indexes,
        "api_endpoints": old_data.get("api_endpoints", {})
    }

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é –≤ {OUTPUT_FILE.name}...")
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    size_mb = OUTPUT_FILE.stat().st_size / (1024 * 1024)
    print(f"  ‚úÖ –†–∞–∑–º–µ—Ä: {size_mb:.2f} MB")

    print("\n‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"üìÅ –ù–æ–≤—ã–π —Ñ–∞–π–ª: {OUTPUT_FILE}")
    print("\n‚ö†Ô∏è  –î–∞–ª–µ–µ –Ω—É–∂–Ω–æ:")
    print("1. –û–±–Ω–æ–≤–∏—Ç—å orchestrator.py –Ω–∞ selfology_core_v2.json")
    print("2. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∑–∞–≥—Ä—É–∑–∫—É")
    print("3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≤—ã–±–æ—Ä –≤–æ–ø—Ä–æ—Å–æ–≤")


if __name__ == "__main__":
    migrate()
