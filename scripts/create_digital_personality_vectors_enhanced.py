"""
–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è Digital Personality –≤ Qdrant (ENHANCED VERSION)

–ù–û–í–û–ï: –î–æ–±–∞–≤–ª—è–µ—Ç –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —á–µ—Ä—Ç—ã (Big Five, dynamic, adaptive, domain-specific)
–≤ payload –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è —Ä–∞–±–æ—Ç—ã AI –∫–æ—É—á–∞

–í–µ–∫—Ç–æ—Ä–∏–∑—É–µ—Ç:
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é (–∏–Ω—Ç–µ—Ä–µ—Å—ã, —Ü–µ–ª–∏, –±–∞—Ä—å–µ—Ä—ã)
- Big Five personality traits
- Dynamic traits (resilience, authenticity, growth_mindset)
- Adaptive traits (stress_level, creative_flow, current_energy, social_battery)
- Domain-specific traits (–ø–æ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –¥–æ–º–µ–Ω–∞–º)
- Psychological analysis
- Quality & processing metadata
"""

import asyncio
import logging
import sys
import json
from pathlib import Path
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
sys.path.append(str(Path(__file__).parent.parent))

from selfology_bot.database import DatabaseService, DigitalPersonalityDAO
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance

# OpenAI –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è embeddings
try:
    from openai import AsyncOpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def get_latest_analysis(db_service: DatabaseService, user_id: int):
    """
    –ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π (—Å–∞–º—ã–π –ø–æ–ª–Ω—ã–π) –∞–Ω–∞–ª–∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    Returns:
        Dict —Å –ø–æ–ª–Ω—ã–º AI –∞–Ω–∞–ª–∏–∑–æ–º –≤–∫–ª—é—á–∞—è personality_traits, psychological_analysis –∏ —Ç.–¥.
    """
    conn = await db_service.pool.acquire()

    try:
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∞–Ω–∞–ª–∏–∑ —Å personality_summary (—Å–∞–º—ã–π –ø–æ–ª–Ω—ã–π)
        result = await conn.fetchrow("""
            SELECT aa.raw_ai_response
            FROM selfology.answer_analysis aa
            JOIN selfology.user_answers_new ua ON aa.user_answer_id = ua.id
            JOIN selfology.onboarding_sessions os ON ua.session_id = os.id
            WHERE os.user_id = $1
              AND aa.raw_ai_response ? 'personality_summary'
            ORDER BY aa.id DESC
            LIMIT 1
        """, user_id)

        if not result:
            logger.warning(f"‚ö†Ô∏è No AI analysis with personality_summary found for user {user_id}")
            return None

        # raw_ai_response —ç—Ç–æ JSONB, –º–æ–∂–µ—Ç –ø—Ä–∏–π—Ç–∏ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ dict
        analysis = result['raw_ai_response']

        # –ï—Å–ª–∏ –ø—Ä–∏—à–ª–∞ —Å—Ç—Ä–æ–∫–∞ - –ø–∞—Ä—Å–∏–º JSON
        if isinstance(analysis, str):
            try:
                analysis = json.loads(analysis)
                logger.info(f"‚úÖ Parsed raw_ai_response from JSON string")
            except json.JSONDecodeError as e:
                logger.error(f"‚ùå Failed to parse raw_ai_response: {e}")
                return None

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ dict
        if not isinstance(analysis, dict):
            logger.error(f"‚ùå raw_ai_response is not a dict after parsing: {type(analysis)}")
            return None

        logger.info(f"‚úÖ Found latest AI analysis with {len(analysis.keys())} top-level keys")
        logger.info(f"   Keys: {', '.join(analysis.keys())}")

        return analysis

    finally:
        await db_service.pool.release(conn)


def safe_extract(obj, *keys, default=None):
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –∫–ª—é—á–µ–π –∏–∑ —Å–ª–æ–≤–∞—Ä—è

    Example:
        safe_extract(data, 'personality_traits', 'big_five', 'openness', default=0.5)
    """
    current = obj
    for key in keys:
        if isinstance(current, dict) and key in current:
            current = current[key]
        else:
            return default
    return current


async def create_digital_personality_vectors(user_id: int):
    """
    –°–æ–∑–¥–∞—ë—Ç –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è Digital Personality —Å –ü–û–õ–ù–û–ô –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π

    –°–æ–∑–¥–∞—ë—Ç 2 —Ç–∏–ø–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤:
    1. Structured vector (1536D) - –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ + –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —á–µ—Ä—Ç—ã –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    2. Narrative vector (3072D) - —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ + –ø–æ–ª–Ω–∞—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è –¥–ª—è AI –∫–æ—É—á–∞
    """

    logger.info("="*80)
    logger.info(f"üîÆ CREATING ENHANCED DIGITAL PERSONALITY VECTORS FOR USER {user_id}")
    logger.info("="*80)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    db_service = DatabaseService(
        host="localhost",
        port=5432,
        user="n8n",
        password="sS67wM+1zMBRFHAW4kj9HwFl5J6+veo7Nirx0/I+oiU=",
        database="n8n"
    )
    await db_service.initialize()

    personality_dao = DigitalPersonalityDAO(db_service)

    # OpenAI client
    import os
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.error("‚ùå OPENAI_API_KEY not found")
        return

    openai_client = AsyncOpenAI(api_key=api_key)

    # Qdrant client
    qdrant_client = QdrantClient(host="localhost", port=6333)

    # 1. –ü–æ–ª—É—á–∞–µ–º Digital Personality (interests, goals, barriers)
    personality = await personality_dao.get_personality(user_id)

    if not personality:
        logger.error(f"‚ùå No digital personality found for user {user_id}")
        return

    logger.info(f"‚úÖ Loaded digital personality")
    logger.info(f"   Completeness: {personality.get('completeness_score', 0):.2%}")

    # 2. –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π AI –∞–Ω–∞–ª–∏–∑ —Å –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ —á–µ—Ä—Ç–∞–º–∏
    analysis = await get_latest_analysis(db_service, user_id)

    if not analysis:
        logger.warning("‚ö†Ô∏è No AI analysis found - will create vectors WITHOUT psychological traits")
        psychological_traits = None
    else:
        psychological_traits = analysis.get('personality_traits', {})
        logger.info(f"‚úÖ Loaded psychological traits")
        logger.info(f"   Big Five: {psychological_traits.get('big_five', {}).keys() if 'big_five' in psychological_traits else 'NOT FOUND'}")
        logger.info(f"   Dynamic traits: {len(psychological_traits.get('dynamic_traits', {}))} traits")
        logger.info(f"   Adaptive traits: {len(psychological_traits.get('adaptive_traits', {}))} traits")
        logger.info(f"   Domain-specific: {len(psychological_traits.get('domain_specific', {}))} domains")

    # === –°–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç ===

    collections_config = [
        {
            "name": "digital_personality_structured",
            "size": 1536,
            "description": "Structured personality data with full psychological traits for precise matching"
        },
        {
            "name": "digital_personality_narrative",
            "size": 3072,
            "description": "Human-readable personality narrative with psychology for AI coach"
        }
    ]

    for config in collections_config:
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é —á–µ—Ä–µ–∑ REST API –Ω–∞–ø—Ä—è–º—É—é
            import requests
            response = requests.get(f"http://localhost:6333/collections/{config['name']}")
            if response.status_code == 200:
                logger.info(f"‚úÖ Collection '{config['name']}' already exists")
                continue
        except Exception as e:
            logger.warning(f"Could not check collection existence: {e}")

        try:
            qdrant_client.create_collection(
                collection_name=config["name"],
                vectors_config=VectorParams(
                    size=config["size"],
                    distance=Distance.COSINE
                )
            )
            logger.info(f"‚úÖ Created collection '{config['name']}' ({config['size']}D)")
        except Exception as create_error:
            if "already exists" in str(create_error).lower():
                logger.info(f"‚úÖ Collection '{config['name']}' already exists")
            else:
                raise

    # === –í–ï–ö–¢–û–† 1: STRUCTURED (1536D) ===
    logger.info("\n" + "="*80)
    logger.info("üìä CREATING ENHANCED STRUCTURED VECTOR (1536D)")
    logger.info("="*80)

    # –°–æ–∑–¥–∞—ë–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å–æ –≤—Å–µ–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
    structured_parts = []

    # –ü–∞—Ä—Å–∏–º JSONB –ø–æ–ª—è
    interests = json.loads(personality.get('interests', '[]')) if isinstance(personality.get('interests'), str) else personality.get('interests', [])
    goals = json.loads(personality.get('goals', '[]')) if isinstance(personality.get('goals'), str) else personality.get('goals', [])
    barriers = json.loads(personality.get('barriers', '[]')) if isinstance(personality.get('barriers'), str) else personality.get('barriers', [])
    skills = json.loads(personality.get('skills', '[]')) if isinstance(personality.get('skills'), str) else personality.get('skills', [])
    relationships = json.loads(personality.get('relationships', '[]')) if isinstance(personality.get('relationships'), str) else personality.get('relationships', [])
    values = json.loads(personality.get('values', '[]')) if isinstance(personality.get('values'), str) else personality.get('values', [])
    health = json.loads(personality.get('health', '[]')) if isinstance(personality.get('health'), str) else personality.get('health', [])
    current_state = json.loads(personality.get('current_state', '[]')) if isinstance(personality.get('current_state'), str) else personality.get('current_state', [])

    # –ò–Ω—Ç–µ—Ä–µ—Å—ã
    if interests:
        active_interests = [i['activity'] for i in interests if i.get('status') == 'active']
        if active_interests:
            structured_parts.append(f"Interests: {', '.join(active_interests)}")

    # –ù–∞–≤—ã–∫–∏
    if skills:
        skill_names = [s['skill'] for s in skills]
        if skill_names:
            structured_parts.append(f"Skills: {', '.join(skill_names)}")

    # –¶–µ–ª–∏
    if goals:
        high_priority_goals = [g['goal'] for g in goals if g.get('priority') == 'high']
        if high_priority_goals:
            structured_parts.append(f"High priority goals: {', '.join(high_priority_goals)}")

        medium_priority_goals = [g['goal'] for g in goals if g.get('priority') == 'medium']
        if medium_priority_goals:
            structured_parts.append(f"Medium priority goals: {', '.join(medium_priority_goals)}")

    # –ë–∞—Ä—å–µ—Ä—ã
    if barriers:
        barrier_texts = [b['barrier'] for b in barriers]
        if barrier_texts:
            structured_parts.append(f"Barriers and fears: {', '.join(barrier_texts)}")

    # –û—Ç–Ω–æ—à–µ–Ω–∏—è
    if relationships:
        important_people = [f"{r['person']} ({r['relationship']})" for r in relationships]
        if important_people:
            structured_parts.append(f"Important people: {', '.join(important_people)}")

    # –¶–µ–Ω–Ω–æ—Å—Ç–∏
    if values:
        value_texts = [v['value'] for v in values]
        if value_texts:
            structured_parts.append(f"Core values: {', '.join(value_texts)}")

    # –ó–¥–æ—Ä–æ–≤—å–µ
    if health:
        health_texts = [f"{h['aspect']}: {h['condition']}" for h in health]
        if health_texts:
            structured_parts.append(f"Health considerations: {'; '.join(health_texts)}")

    # –ù–û–í–û–ï: –î–æ–±–∞–≤–ª—è–µ–º –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —á–µ—Ä—Ç—ã –≤ —Ç–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    if psychological_traits:
        # Big Five
        big_five = psychological_traits.get('big_five', {})
        if big_five:
            traits_text = ", ".join([f"{trait}: {score:.2f}" for trait, score in big_five.items()])
            structured_parts.append(f"Big Five traits: {traits_text}")

        # Dynamic traits
        dynamic = psychological_traits.get('dynamic_traits', {})
        if dynamic:
            dynamic_text = ", ".join([f"{trait}: {score:.2f}" for trait, score in dynamic.items()])
            structured_parts.append(f"Dynamic traits: {dynamic_text}")

        # Adaptive traits
        adaptive = psychological_traits.get('adaptive_traits', {})
        if adaptive:
            adaptive_text = ", ".join([f"{trait}: {score:.2f}" for trait, score in adaptive.items()])
            structured_parts.append(f"Adaptive traits: {adaptive_text}")

    structured_text = " | ".join(structured_parts)

    logger.info(f"\nüìù Enhanced structured text to vectorize:")
    logger.info(f"   {structured_text[:300]}...")

    # –°–æ–∑–¥–∞—ë–º embedding
    response = await openai_client.embeddings.create(
        model="text-embedding-3-small",
        input=structured_text
    )

    structured_vector = response.data[0].embedding

    logger.info(f"‚úÖ Created 1536D embedding")

    # –ù–û–í–û–ï: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π payload —Å –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ —á–µ—Ä—Ç–∞–º–∏
    payload = {
        "user_id": user_id,

        # –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
        "interests": interests,
        "goals": goals,
        "barriers": barriers,
        "skills": skills,
        "relationships": relationships,
        "values": values,
        "health": health,
        "completeness_score": personality.get('completeness_score', 0),
        "last_updated": personality.get('last_updated').isoformat() if personality.get('last_updated') else None,
        "structured_text": structured_text
    }

    # –ù–û–í–û–ï: –î–æ–±–∞–≤–ª—è–µ–º –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —á–µ—Ä—Ç—ã
    if psychological_traits:
        payload["personality_traits"] = {
            "big_five": psychological_traits.get('big_five', {}),
            "dynamic_traits": psychological_traits.get('dynamic_traits', {}),
            "adaptive_traits": psychological_traits.get('adaptive_traits', {}),
            "domain_specific": psychological_traits.get('domain_specific', {})
        }

        logger.info(f"‚úÖ Added psychological traits to payload:")
        logger.info(f"   - Big Five: {len(payload['personality_traits']['big_five'])} traits")
        logger.info(f"   - Dynamic: {len(payload['personality_traits']['dynamic_traits'])} traits")
        logger.info(f"   - Adaptive: {len(payload['personality_traits']['adaptive_traits'])} traits")
        logger.info(f"   - Domain-specific: {len(payload['personality_traits']['domain_specific'])} domains")

    # –ù–û–í–û–ï: –î–æ–±–∞–≤–ª—è–µ–º psychological_analysis –µ—Å–ª–∏ –µ—Å—Ç—å
    if analysis and 'psychological_analysis' in analysis:
        payload["psychological_analysis"] = analysis['psychological_analysis']
        logger.info(f"‚úÖ Added psychological_analysis to payload")

    # –ù–û–í–û–ï: –î–æ–±–∞–≤–ª—è–µ–º quality_metadata –µ—Å–ª–∏ –µ—Å—Ç—å
    if analysis and 'quality_metadata' in analysis:
        payload["quality_metadata"] = analysis['quality_metadata']
        logger.info(f"‚úÖ Added quality_metadata to payload")

    # –ù–û–í–û–ï: –î–æ–±–∞–≤–ª—è–µ–º processing_metadata –µ—Å–ª–∏ –µ—Å—Ç—å
    if analysis and 'processing_metadata' in analysis:
        payload["processing_metadata"] = analysis['processing_metadata']
        logger.info(f"‚úÖ Added processing_metadata to payload")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
    qdrant_client.upsert(
        collection_name="digital_personality_structured",
        points=[
            PointStruct(
                id=user_id,
                vector=structured_vector,
                payload=payload
            )
        ]
    )

    logger.info(f"‚úÖ Saved enhanced structured vector to Qdrant")

    # === –í–ï–ö–¢–û–† 2: NARRATIVE (3072D) ===
    logger.info("\n" + "="*80)
    logger.info("üìñ CREATING ENHANCED NARRATIVE VECTOR (3072D) FOR AI COACH")
    logger.info("="*80)

    # –°–æ–∑–¥–∞—ë–º —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ª–∏—á–Ω–æ—Å—Ç–∏ –¥–ª—è AI –∫–æ—É—á–∞
    narrative_parts = []

    # –í—Å—Ç—É–ø–ª–µ–Ω–∏–µ
    narrative_parts.append("This is a comprehensive personality profile of the user with detailed psychological analysis.")

    # –ù–û–í–û–ï: –ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —á–µ—Ä—Ç—ã –≤ narrative
    if psychological_traits:
        # Big Five
        big_five = psychological_traits.get('big_five', {})
        if big_five:
            bf_text = ", ".join([
                f"{trait}: {score:.2f} ({'high' if score > 0.7 else 'medium' if score > 0.4 else 'low'})"
                for trait, score in big_five.items()
            ])
            narrative_parts.append(f"Big Five personality traits: {bf_text}.")

        # Dynamic traits
        dynamic = psychological_traits.get('dynamic_traits', {})
        if dynamic:
            dynamic_text = ", ".join([f"{trait}: {score:.2f}" for trait, score in dynamic.items()])
            narrative_parts.append(f"Dynamic psychological traits including: {dynamic_text}.")

        # Adaptive traits
        adaptive = psychological_traits.get('adaptive_traits', {})
        if adaptive:
            adaptive_text = ", ".join([f"{trait}: {score:.2f}" for trait, score in adaptive.items()])
            narrative_parts.append(f"Current adaptive state: {adaptive_text}.")

    # –ò–Ω—Ç–µ—Ä–µ—Å—ã –∏ —É–≤–ª–µ—á–µ–Ω–∏—è
    if interests:
        active = [i['activity'] for i in interests if i.get('status') == 'active']
        inactive = [i['activity'] for i in interests if i.get('status') == 'inactive']

        if active:
            narrative_parts.append(f"The user is actively interested in: {', '.join(active)}.")
        if inactive:
            narrative_parts.append(f"Previously interested but no longer active: {', '.join(inactive)}.")

    # –ù–∞–≤—ã–∫–∏
    if skills:
        skill_descriptions = []
        for skill in skills:
            level = skill.get('level', 'unknown')
            name = skill['skill']
            skill_descriptions.append(f"{name} (level: {level})")

        if skill_descriptions:
            narrative_parts.append(f"Skills and abilities: {', '.join(skill_descriptions)}.")

    # –¶–µ–ª–∏ –∏ –∞–º–±–∏—Ü–∏–∏
    if goals:
        long_term = [g['goal'] for g in goals if g.get('type') == 'long_term']
        short_term = [g['goal'] for g in goals if g.get('type') == 'short_term']

        if long_term:
            narrative_parts.append(f"Long-term aspirations: {', '.join(long_term)}.")
        if short_term:
            narrative_parts.append(f"Short-term goals: {', '.join(short_term)}.")

    # –ë–∞—Ä—å–µ—Ä—ã –∏ —Å—Ç—Ä–∞—Ö–∏
    if barriers:
        barrier_descriptions = []
        for barrier in barriers:
            b_type = barrier.get('type', 'unknown')
            impact = barrier.get('impact', '')
            text = barrier['barrier']
            barrier_descriptions.append(f"{text} ({b_type}, impact: {impact})")

        if barrier_descriptions:
            narrative_parts.append(f"Current barriers and challenges: {'; '.join(barrier_descriptions)}.")

    # –í–∞–∂–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è
    if relationships:
        rel_descriptions = [f"{r['person']} ({r['relationship']})" for r in relationships]
        if rel_descriptions:
            narrative_parts.append(f"Important relationships: {', '.join(rel_descriptions)}. These people matter to the user.")

    # –¶–µ–Ω–Ω–æ—Å—Ç–∏
    if values:
        value_descriptions = []
        for value in values:
            v_text = value['value']
            context = value.get('context', '')
            value_descriptions.append(f"{v_text}" + (f" in {context}" if context else ""))

        if value_descriptions:
            narrative_parts.append(f"Core values and principles: {', '.join(value_descriptions)}.")

    # –ó–¥–æ—Ä–æ–≤—å–µ
    if health:
        health_descriptions = []
        for h in health:
            aspect = h['aspect']
            condition = h['condition']
            impact = h.get('impact', '')
            health_descriptions.append(f"{aspect}: {condition} (impact: {impact})")

        if health_descriptions:
            narrative_parts.append(f"Health and wellbeing considerations: {'; '.join(health_descriptions)}.")

    # –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    if current_state:
        state_descriptions = [f"{s['activity']} ({s.get('status', 'unknown')})" for s in current_state]
        if state_descriptions:
            narrative_parts.append(f"Current activities: {', '.join(state_descriptions)}.")

    # –ù–û–í–û–ï: Psychological analysis insights
    if analysis and 'psychological_analysis' in analysis:
        psych = analysis['psychological_analysis']
        insights = psych.get('insights', [])
        if insights:
            narrative_parts.append(f"Key psychological insights: {'; '.join(insights)}.")

    # –ó–∞–∫–ª—é—á–µ–Ω–∏–µ
    narrative_parts.append(f"This profile is based on {personality.get('total_answers_analyzed', 0)} analyzed answers with {personality.get('completeness_score', 0):.0%} completeness.")

    narrative_text = " ".join(narrative_parts)

    logger.info(f"\nüìù Enhanced narrative text for AI coach:")
    logger.info(f"   {narrative_text[:400]}...")

    # –°–æ–∑–¥–∞—ë–º embedding (3072D)
    response = await openai_client.embeddings.create(
        model="text-embedding-3-large",
        input=narrative_text
    )

    narrative_vector = response.data[0].embedding

    logger.info(f"‚úÖ Created 3072D embedding")

    # –ù–û–í–û–ï: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π payload –¥–ª—è narrative
    narrative_payload = {
        "user_id": user_id,
        "narrative": narrative_text,
        "structured_data": {
            "interests": interests,
            "goals": goals,
            "barriers": barriers,
            "skills": skills,
            "relationships": relationships,
            "values": values,
            "health": health,
            "current_state": current_state
        },
        "completeness_score": personality.get('completeness_score', 0),
        "total_answers_analyzed": personality.get('total_answers_analyzed', 0),
        "last_updated": personality.get('last_updated').isoformat() if personality.get('last_updated') else None
    }

    # –ù–û–í–û–ï: –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
    if psychological_traits:
        narrative_payload["personality_traits"] = {
            "big_five": psychological_traits.get('big_five', {}),
            "dynamic_traits": psychological_traits.get('dynamic_traits', {}),
            "adaptive_traits": psychological_traits.get('adaptive_traits', {}),
            "domain_specific": psychological_traits.get('domain_specific', {})
        }

    if analysis and 'psychological_analysis' in analysis:
        narrative_payload["psychological_analysis"] = analysis['psychological_analysis']

    if analysis and 'quality_metadata' in analysis:
        narrative_payload["quality_metadata"] = analysis['quality_metadata']

    if analysis and 'processing_metadata' in analysis:
        narrative_payload["processing_metadata"] = analysis['processing_metadata']

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Qdrant
    qdrant_client.upsert(
        collection_name="digital_personality_narrative",
        points=[
            PointStruct(
                id=user_id,
                vector=narrative_vector,
                payload=narrative_payload
            )
        ]
    )

    logger.info(f"‚úÖ Saved enhanced narrative vector to Qdrant")

    # === –ü–†–û–í–ï–†–ö–ê ===
    logger.info("\n" + "="*80)
    logger.info("üîç VERIFICATION")
    logger.info("="*80)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤–µ–∫—Ç–æ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏—Å—å —á–µ—Ä–µ–∑ curl (get_collection –∏–º–µ–µ—Ç –±–∞–≥ —Å pydantic)
    try:
        import requests
        for collection in ["digital_personality_structured", "digital_personality_narrative"]:
            response = requests.post(
                f"http://localhost:6333/collections/{collection}/points/scroll",
                json={"limit": 1, "with_payload": True, "with_vector": False, "filter": {"must": [{"key": "user_id", "match": {"value": user_id}}]}}
            )
            if response.status_code == 200:
                data = response.json()
                if data['result']['points']:
                    point = data['result']['points'][0]
                    logger.info(f"‚úÖ {collection}: Vector exists")
                    logger.info(f"   Payload keys: {list(point['payload'].keys())}")
                else:
                    logger.error(f"‚ùå {collection}: Vector NOT found")
            else:
                logger.error(f"‚ùå {collection}: Failed to verify (HTTP {response.status_code})")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Verification failed (non-critical): {e}")
        logger.info(f"   Vectors were saved successfully, verification just didn't work")

    logger.info("\n" + "="*80)
    logger.info("üéâ ENHANCED DIGITAL PERSONALITY VECTORS CREATED SUCCESSFULLY!")
    logger.info("="*80)
    logger.info("\nüí° Now AI coach has access to:")
    logger.info("   ‚úÖ Interests, goals, barriers, values (as before)")
    logger.info("   ‚úÖ Big Five personality traits")
    logger.info("   ‚úÖ Dynamic traits (resilience, authenticity, growth_mindset...)")
    logger.info("   ‚úÖ Adaptive traits (stress_level, creative_flow, social_battery...)")
    logger.info("   ‚úÖ Domain-specific traits (per psychological domain)")
    logger.info("   ‚úÖ Psychological analysis insights")
    logger.info("   ‚úÖ Quality & processing metadata")
    logger.info("="*80)

    await db_service.close()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Create ENHANCED digital personality vectors with full psychology')
    parser.add_argument('--user-id', type=int, required=True, help='User ID')
    args = parser.parse_args()

    asyncio.run(create_digital_personality_vectors(args.user_id))
