"""
–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –ª–∏—á–Ω–æ—Å—Ç–∏ –≤ Qdrant –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∞–Ω–∞–ª–∏–∑–æ–≤

–ë–µ—Ä—ë—Ç –≤—Å–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –∏ —Å–æ–∑–¥–∞—ë—Ç –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
"""

import asyncio
import logging
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
sys.path.append(str(Path(__file__).parent.parent))

from selfology_bot.database import DatabaseService, OnboardingDAO
from selfology_bot.analysis import EmbeddingCreator

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def create_vectors_for_analyzed_answers(user_id: int = None):
    """
    –°–æ–∑–¥–∞—ë—Ç –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–æ–≤

    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–µ—Å–ª–∏ None - –≤—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏)
    """

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    logger.info("üîß Initializing components...")

    db_service = DatabaseService(
        host="localhost",
        port=5432,
        user="n8n",
        password="sS67wM+1zMBRFHAW4kj9HwFl5J6+veo7Nirx0/I+oiU=",
        database="n8n"
    )
    await db_service.initialize()

    dao = OnboardingDAO(db_service)
    embedding_creator = EmbeddingCreator()

    # Setup Qdrant collections
    logger.info("üîß Setting up Qdrant collections...")
    setup_success = await embedding_creator.setup_qdrant_collections()
    if not setup_success:
        logger.error("‚ùå Failed to setup Qdrant collections")
        return

    logger.info("‚úÖ Components initialized")

    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∞–Ω–∞–ª–∏–∑—ã
    async with db_service.get_connection() as conn:
        if user_id:
            query = """
                SELECT
                    aa.id as analysis_id,
                    aa.user_answer_id,
                    aa.trait_scores,
                    aa.psychological_insights,
                    aa.emotional_state,
                    aa.ai_model_used,
                    aa.quality_score,
                    aa.confidence_score,
                    ua.session_id,
                    ua.question_json_id,
                    ua.raw_answer,
                    os.user_id
                FROM selfology.answer_analysis aa
                JOIN selfology.user_answers_new ua ON aa.user_answer_id = ua.id
                JOIN selfology.onboarding_sessions os ON ua.session_id = os.id
                WHERE os.user_id = $1 AND aa.trait_scores IS NOT NULL
                ORDER BY aa.id
            """
            rows = await conn.fetch(query, user_id)
        else:
            query = """
                SELECT
                    aa.id as analysis_id,
                    aa.user_answer_id,
                    aa.trait_scores,
                    aa.psychological_insights,
                    aa.emotional_state,
                    aa.ai_model_used,
                    aa.quality_score,
                    aa.confidence_score,
                    ua.session_id,
                    ua.question_json_id,
                    ua.raw_answer,
                    os.user_id
                FROM selfology.answer_analysis aa
                JOIN selfology.user_answers_new ua ON aa.user_answer_id = ua.id
                JOIN selfology.onboarding_sessions os ON ua.session_id = os.id
                WHERE aa.trait_scores IS NOT NULL
                ORDER BY aa.id
            """
            rows = await conn.fetch(query)

    total = len(rows)
    logger.info(f"üìä Found {total} analyses to vectorize")

    if total == 0:
        logger.info("‚úÖ No analyses found")
        return

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
    user_analyses = {}
    for row in rows:
        uid = row['user_id']
        if uid not in user_analyses:
            user_analyses[uid] = []
        user_analyses[uid].append(dict(row))

    logger.info(f"üë• Found {len(user_analyses)} unique users")

    # –°–æ–∑–¥–∞—ë–º –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    for uid, analyses in user_analyses.items():
        logger.info(f"\n{'='*60}")
        logger.info(f"üë§ Processing user {uid} ({len(analyses)} analyses)")

        # –ê–∫–∫—É–º—É–ª–∏—Ä—É–µ–º –≤—Å–µ trait_scores –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è
        accumulated_traits = {
            "big_five": {},
            "dynamic_traits": {},
            "adaptive_traits": {},
            "domain_specific": {}
        }

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–Ω—Å–∞–π—Ç—ã
        all_insights = []

        for idx, analysis in enumerate(analyses, 1):
            trait_scores = analysis['trait_scores']

            # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ (–Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å), –ø–∞—Ä—Å–∏–º
            if isinstance(trait_scores, str):
                import json
                trait_scores = json.loads(trait_scores)

            # –ê–∫–∫—É–º—É–ª–∏—Ä—É–µ–º —á–µ—Ä—Ç—ã (—É—Å—Ä–µ–¥–Ω—è–µ–º)
            for category in accumulated_traits.keys():
                if category in trait_scores and isinstance(trait_scores[category], dict):
                    for trait, value in trait_scores[category].items():
                        if isinstance(value, dict):
                            # domain_specific
                            if trait not in accumulated_traits[category]:
                                accumulated_traits[category][trait] = {}
                            for subtrait, subvalue in value.items():
                                if isinstance(subvalue, (int, float)):
                                    if subtrait not in accumulated_traits[category][trait]:
                                        accumulated_traits[category][trait][subtrait] = []
                                    accumulated_traits[category][trait][subtrait].append(subvalue)
                        elif isinstance(value, (int, float)):
                            if trait not in accumulated_traits[category]:
                                accumulated_traits[category][trait] = []
                            accumulated_traits[category][trait].append(value)

            # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Å–∞–π—Ç—ã
            if analysis['psychological_insights']:
                insights = analysis['psychological_insights']
                # insights —ç—Ç–æ —É–∂–µ dict (JSONB), –Ω–µ —Å—Ç—Ä–æ–∫–∞
                if isinstance(insights, dict):
                    if 'main' in insights:
                        all_insights.append(insights['main'])
                    elif insights:  # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—å —á—Ç–æ-—Ç–æ
                        all_insights.append(str(insights))
                elif isinstance(insights, str):
                    all_insights.append(insights)

        # –£—Å—Ä–µ–¥–Ω—è–µ–º —á–µ—Ä—Ç—ã
        final_traits = {
            "big_five": {},
            "dynamic_traits": {},
            "adaptive_traits": {},
            "domain_specific": {}
        }

        for category in accumulated_traits.keys():
            for trait, values in accumulated_traits[category].items():
                if isinstance(values, dict):
                    final_traits[category][trait] = {}
                    for subtrait, subvalues in values.items():
                        if subvalues:
                            final_traits[category][trait][subtrait] = sum(subvalues) / len(subvalues)
                elif isinstance(values, list) and values:
                    final_traits[category][trait] = sum(values) / len(values)

        # –°–æ–∑–¥–∞—ë–º personality narrative –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        narrative_parts = []

        # –î–æ–±–∞–≤–ª—è–µ–º Big Five –æ–ø–∏—Å–∞–Ω–∏–µ
        bf = final_traits['big_five']
        narrative_parts.append(f"–õ–∏—á–Ω–æ—Å—Ç—å —Å –≤—ã—Å–æ–∫–æ–π –æ—Ç–∫—Ä—ã—Ç–æ—Å—Ç—å—é ({bf.get('openness', 0.5):.2f}), "
                               f"–¥–æ–±—Ä–æ—Å–æ–≤–µ—Å—Ç–Ω–æ—Å—Ç—å—é ({bf.get('conscientiousness', 0.5):.2f}), "
                               f"—ç–∫—Å—Ç—Ä–∞–≤–µ—Ä—Å–∏–µ–π ({bf.get('extraversion', 0.5):.2f}), "
                               f"–¥–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é ({bf.get('agreeableness', 0.5):.2f}), "
                               f"–∏ –Ω–µ–π—Ä–æ—Ç–∏–∑–º–æ–º ({bf.get('neuroticism', 0.5):.2f}).")

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã
        if all_insights:
            narrative_parts.append("–ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã: " + "; ".join(all_insights[:3]))

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —á–µ—Ä—Ç—ã
        dt = final_traits['dynamic_traits']
        if dt:
            top_dynamic = sorted(dt.items(), key=lambda x: x[1], reverse=True)[:3]
            traits_str = ", ".join([f"{k} ({v:.2f})" for k, v in top_dynamic])
            narrative_parts.append(f"–í—ã—Ä–∞–∂–µ–Ω–Ω—ã–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ —á–µ—Ä—Ç—ã: {traits_str}.")

        personality_narrative = " ".join(narrative_parts)

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        avg_confidence = sum(a['confidence_score'] for a in analyses) / len(analyses)
        avg_quality = sum(a['quality_score'] for a in analyses) / len(analyses)

        # –°–æ–∑–¥–∞—ë–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π analysis_result
        analysis_result = {
            "personality_summary": {
                "narrative": personality_narrative,
                "short_description": f"–ü—Ä–æ—Ñ–∏–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ {len(analyses)} –æ—Ç–≤–µ—Ç–æ–≤",
                "key_traits": list(bf.keys())
            },
            "personality_traits": final_traits,  # –ù—É–∂–Ω–æ –¥–ª—è EmbeddingCreator
            "trait_extraction": {
                "version": "2.0",
                "traits": final_traits,
                "assessment_metadata": {
                    "total_analyses": len(analyses),
                    "confidence_avg": avg_confidence,
                    "quality_avg": avg_quality
                }
            },
            "core_analysis": {
                "insights": {
                    "main": f"–ê–∫–∫—É–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã –∏–∑ {len(analyses)} –æ—Ç–≤–µ—Ç–æ–≤",
                    "patterns": all_insights[:5]  # –ü–µ—Ä–≤—ã–µ 5
                }
            },
            "processing_metadata": {
                "model_used": analyses[-1]['ai_model_used'],
                "analyses_count": len(analyses)
            },
            "quality_metadata": {  # –ù—É–∂–Ω–æ –¥–ª—è EmbeddingCreator
                "overall_reliability": avg_quality,
                "confidence": avg_confidence
            }
        }

        logger.info(f"   üìä Accumulated {len(analyses)} analyses into personality profile")
        logger.info(f"   üß¨ Big Five traits: {len(final_traits['big_five'])}")
        logger.info(f"   üî¨ Dynamic traits: {len(final_traits['dynamic_traits'])}")

        # –°–æ–∑–¥–∞—ë–º –≤–µ–∫—Ç–æ—Ä—ã
        try:
            vector_success = await embedding_creator.create_personality_vector(
                user_id=uid,
                analysis_result=analysis_result,
                is_update=False  # –ü–µ—Ä–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ
            )

            if vector_success:
                logger.info(f"   ‚úÖ Vectors created successfully for user {uid}")
            else:
                logger.error(f"   ‚ùå Failed to create vectors for user {uid}")

        except Exception as e:
            logger.error(f"   ‚ùå Error creating vectors for user {uid}: {e}", exc_info=True)

    logger.info(f"\n{'='*60}")
    logger.info(f"üéâ Vector creation completed!")
    logger.info(f"   Processed {len(user_analyses)} users")

    await db_service.close()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Create Qdrant vectors from analyses')
    parser.add_argument('--user-id', type=int, help='Specific user ID')
    args = parser.parse_args()

    asyncio.run(create_vectors_for_analyzed_answers(args.user_id))
