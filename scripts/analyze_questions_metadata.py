#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π
"""

import json
from pathlib import Path
from collections import defaultdict, Counter
from typing import Dict, List, Any

def analyze_questions():
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""

    project_root = Path(__file__).parent.parent
    questions_file = project_root / "intelligent_question_core" / "data" / "enhanced_questions.json"

    with open(questions_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
        questions = data.get("questions", [])

    total = len(questions)

    # –°—á–µ—Ç—á–∏–∫–∏
    domains = Counter()
    depth_levels = Counter()
    energy_dynamics = Counter()
    journey_stages = Counter()
    recommended_models = Counter()

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ psychology –ø–æ–ª—è–º
    complexity_dist = defaultdict(int)
    emotional_weight_dist = defaultdict(int)
    insight_potential_dist = defaultdict(int)
    safety_level_dist = defaultdict(int)
    trust_requirement_dist = defaultdict(int)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    for q in questions:
        cls = q.get("classification", {})
        psy = q.get("psychology", {})
        hints = q.get("processing_hints", {})

        domains[cls.get("domain")] += 1
        depth_levels[cls.get("depth_level")] += 1
        energy_dynamics[cls.get("energy_dynamic")] += 1
        journey_stages[cls.get("journey_stage")] += 1
        recommended_models[hints.get("recommended_model")] += 1

        complexity_dist[psy.get("complexity")] += 1
        emotional_weight_dist[psy.get("emotional_weight")] += 1
        insight_potential_dist[psy.get("insight_potential")] += 1
        safety_level_dist[psy.get("safety_level")] += 1
        trust_requirement_dist[psy.get("trust_requirement")] += 1

    # –í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞
    print("=" * 80)
    print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ú–ï–¢–ê–î–ê–ù–ù–´–• –í–û–ü–†–û–°–û–í")
    print("=" * 80)
    print(f"\n–í—Å–µ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {total}\n")

    # Domains
    print("üè∑Ô∏è  PSYCHOLOGICAL DOMAINS:")
    for domain, count in sorted(domains.items(), key=lambda x: x[1], reverse=True):
        pct = count / total * 100
        bar = "‚ñà" * int(pct / 2)
        print(f"  {domain:20s} {count:3d} ({pct:5.1f}%) {bar}")

    # Depth levels
    print("\nüìä DEPTH LEVELS:")
    depth_order = ["SURFACE", "CONSCIOUS", "EDGE", "SHADOW", "CORE"]
    for depth in depth_order:
        count = depth_levels.get(depth, 0)
        pct = count / total * 100 if count else 0
        bar = "‚ñà" * int(pct / 2)
        print(f"  {depth:20s} {count:3d} ({pct:5.1f}%) {bar}")

    # Energy dynamics
    print("\n‚ö° ENERGY DYNAMICS:")
    for energy, count in sorted(energy_dynamics.items(), key=lambda x: x[1], reverse=True):
        pct = count / total * 100
        bar = "‚ñà" * int(pct / 2)
        print(f"  {energy:20s} {count:3d} ({pct:5.1f}%) {bar}")

    # Journey stages
    print("\nüöÄ JOURNEY STAGES:")
    for stage, count in sorted(journey_stages.items(), key=lambda x: x[1], reverse=True):
        pct = count / total * 100
        bar = "‚ñà" * int(pct / 2)
        print(f"  {stage:20s} {count:3d} ({pct:5.1f}%) {bar}")

    # Recommended models
    print("\nü§ñ RECOMMENDED AI MODELS:")
    for model, count in sorted(recommended_models.items(), key=lambda x: x[1], reverse=True):
        pct = count / total * 100
        bar = "‚ñà" * int(pct / 2)
        print(f"  {model:20s} {count:3d} ({pct:5.1f}%) {bar}")

    # Psychology metrics (1-5 scale)
    print("\nüß† PSYCHOLOGY METRICS (1-5 scale):")

    metrics = [
        ("Complexity", complexity_dist),
        ("Emotional Weight", emotional_weight_dist),
        ("Insight Potential", insight_potential_dist),
        ("Safety Level", safety_level_dist),
        ("Trust Requirement", trust_requirement_dist)
    ]

    for name, dist in metrics:
        print(f"\n  {name}:")
        avg = sum(k * v for k, v in dist.items()) / total
        for i in range(1, 6):
            count = dist.get(i, 0)
            pct = count / total * 100
            bar = "‚ñà" * int(pct / 2)
            print(f"    {i}: {count:3d} ({pct:5.1f}%) {bar}")
        print(f"    –°—Ä–µ–¥–Ω–µ–µ: {avg:.2f}")

    # –í—ã–≤–æ–¥—ã
    print("\n" + "=" * 80)
    print("üéØ –ö–õ–Æ–ß–ï–í–´–ï –í–´–í–û–î–´:")
    print("=" * 80)

    # –¢–æ–ø 3 –¥–æ–º–µ–Ω–∞
    top_domains = sorted(domains.items(), key=lambda x: x[1], reverse=True)[:3]
    print(f"\n‚úÖ –¢–æ–ø-3 –¥–æ–º–µ–Ω–∞:")
    for domain, count in top_domains:
        print(f"   ‚Ä¢ {domain}: {count} –≤–æ–ø—Ä–æ—Å–æ–≤ ({count/total*100:.1f}%)")

    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥–ª—É–±–∏–Ω–µ
    deep_questions = depth_levels.get("SHADOW", 0) + depth_levels.get("CORE", 0)
    print(f"\n‚úÖ –ì–ª—É–±–æ–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã (SHADOW + CORE): {deep_questions} ({deep_questions/total*100:.1f}%)")

    # –ë–∞–ª–∞–Ω—Å —ç–Ω–µ—Ä–≥–∏–∏
    heavy = energy_dynamics.get("HEAVY", 0)
    healing = energy_dynamics.get("HEALING", 0)
    print(f"\n‚úÖ –≠–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–π –±–∞–ª–∞–Ω—Å:")
    print(f"   ‚Ä¢ HEAVY (—Ç—è–∂–µ–ª—ã–µ): {heavy} ({heavy/total*100:.1f}%)")
    print(f"   ‚Ä¢ HEALING (–∏—Å—Ü–µ–ª—è—é—â–∏–µ): {healing} ({healing/total*100:.1f}%)")
    if healing > 0:
        print(f"   ‚Ä¢ –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {heavy/healing:.2f} : 1")
    else:
        print(f"   ‚Ä¢ ‚ö†Ô∏è –ù–ï–¢ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å energy_dynamic=HEALING!")

    # AI –º–æ–¥–µ–ª–∏
    claude_count = recommended_models.get("claude-3.5-sonnet", 0)
    print(f"\n‚úÖ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ AI –º–æ–¥–µ–ª–µ–π:")
    print(f"   ‚Ä¢ Claude Sonnet: {claude_count} ({claude_count/total*100:.1f}%) - –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤")
    print(f"   ‚Ä¢ GPT-4o: {recommended_models.get('gpt-4o', 0)} ({recommended_models.get('gpt-4o', 0)/total*100:.1f}%)")
    print(f"   ‚Ä¢ GPT-4o-mini: {recommended_models.get('gpt-4o-mini', 0)} ({recommended_models.get('gpt-4o-mini', 0)/total*100:.1f}%)")

    print("\n" + "=" * 80)


if __name__ == "__main__":
    analyze_questions()
