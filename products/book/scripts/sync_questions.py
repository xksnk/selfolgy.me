#!/usr/bin/env python3
"""
–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –≤–æ–ø—Ä–æ—Å–æ–≤ –º–µ–∂–¥—É –±–∞–∑–∞–º–∏.

v2 (–∫–Ω–∏–≥–∏) ‚Üí core (–æ–Ω–±–æ—Ä–¥–∏–Ω–≥)

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python sync_questions.py --dry-run    # –ü–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –∏–∑–º–µ–Ω–∏—Ç—Å—è
    python sync_questions.py --apply      # –ü—Ä–∏–º–µ–Ω–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è
"""

import json
import argparse
from pathlib import Path
from difflib import SequenceMatcher
from datetime import datetime

PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
DATA_DIR = PROJECT_ROOT / "intelligent_question_core/data"

V2_FILE = DATA_DIR / "selfology_programs_v2.json"
CORE_FILE = DATA_DIR / "selfology_intelligent_core.json"


def normalize_text(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    return text.lower().strip().replace("—ë", "–µ")


def similarity(a: str, b: str) -> float:
    """–ü–æ—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö —Å—Ç—Ä–æ–∫ (0-1)"""
    return SequenceMatcher(None, normalize_text(a), normalize_text(b)).ratio()


def load_v2_questions() -> dict:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ v2 (–∫–Ω–∏–≥–∏)"""
    with open(V2_FILE, encoding="utf-8") as f:
        data = json.load(f)

    questions = {}
    for prog in data["programs"]:
        for block in prog["blocks"]:
            for q in block["questions"]:
                questions[q["id"]] = {
                    "text": q["text"],
                    "program": prog["name"],
                    "block": block["name"],
                    "v2_id": q["id"]
                }
    return questions


def load_core_questions() -> dict:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ core (–æ–Ω–±–æ—Ä–¥–∏–Ω–≥)"""
    with open(CORE_FILE, encoding="utf-8") as f:
        data = json.load(f)

    return {q["id"]: q for q in data["questions"]}


def find_matches(v2_questions: dict, core_questions: dict, threshold: float = 0.85) -> list:
    """–ù–∞–π—Ç–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –º–µ–∂–¥—É –±–∞–∑–∞–º–∏ –ø–æ —Ç–µ–∫—Å—Ç—É"""
    matches = []

    # –ò–Ω–¥–µ–∫—Å core –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É
    core_by_text = {}
    for core_id, core_q in core_questions.items():
        norm = normalize_text(core_q["text"])
        core_by_text[norm] = (core_id, core_q)

    for v2_id, v2_data in v2_questions.items():
        v2_norm = normalize_text(v2_data["text"])

        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if v2_norm in core_by_text:
            core_id, core_q = core_by_text[v2_norm]
            matches.append({
                "v2_id": v2_id,
                "core_id": core_id,
                "v2_text": v2_data["text"],
                "core_text": core_q["text"],
                "similarity": 1.0,
                "program": v2_data["program"],
                "block": v2_data["block"],
                "needs_update": v2_data["text"] != core_q["text"]  # –†–∞–∑–Ω—ã–π —Ä–µ–≥–∏—Å—Ç—Ä/–ø—Ä–æ–±–µ–ª—ã
            })
            continue

        # Fuzzy matching
        best_match = None
        best_sim = 0
        for core_id, core_q in core_questions.items():
            sim = similarity(v2_data["text"], core_q["text"])
            if sim > best_sim and sim >= threshold:
                best_sim = sim
                best_match = (core_id, core_q)

        if best_match:
            core_id, core_q = best_match
            matches.append({
                "v2_id": v2_id,
                "core_id": core_id,
                "v2_text": v2_data["text"],
                "core_text": core_q["text"],
                "similarity": best_sim,
                "program": v2_data["program"],
                "block": v2_data["block"],
                "needs_update": True
            })

    return matches


def show_sync_plan(matches: list):
    """–ü–æ–∫–∞–∑–∞—Ç—å –ø–ª–∞–Ω —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏"""
    updates = [m for m in matches if m["needs_update"] and m["v2_text"] != m["core_text"]]
    exact = [m for m in matches if m["similarity"] == 1.0]
    fuzzy = [m for m in matches if m["similarity"] < 1.0]

    print("‚ïê" * 60)
    print("üìä –ü–õ–ê–ù –°–ò–ù–•–†–û–ù–ò–ó–ê–¶–ò–ò")
    print("‚ïê" * 60)
    print(f"\n‚úÖ –¢–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(exact)}")
    print(f"üîç Fuzzy —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(fuzzy)}")
    print(f"‚úèÔ∏è  –¢—Ä–µ–±—É—é—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {len(updates)}")

    if updates:
        print("\n" + "‚îÄ" * 60)
        print("üìù –ò–ó–ú–ï–ù–ï–ù–ò–Ø –¢–ï–ö–°–¢–ê (v2 ‚Üí core):")
        print("‚îÄ" * 60)

        for i, m in enumerate(updates[:20], 1):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 20
            print(f"\n{i}. [{m['program']} ‚Üí {m['block']}]")
            print(f"   Similarity: {m['similarity']:.0%}")
            print(f"   CORE: {m['core_text'][:70]}...")
            print(f"   ‚Üí V2: {m['v2_text'][:70]}...")

        if len(updates) > 20:
            print(f"\n   ... –∏ –µ—â—ë {len(updates) - 20} –∏–∑–º–µ–Ω–µ–Ω–∏–π")

    return updates


def apply_sync(matches: list, dry_run: bool = True):
    """–ü—Ä–∏–º–µ–Ω–∏—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é"""
    updates = [m for m in matches if m["needs_update"] and m["v2_text"] != m["core_text"]]

    if not updates:
        print("\n‚úÖ –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è")
        return

    if dry_run:
        print(f"\n‚ö†Ô∏è  DRY RUN: {len(updates)} –∏–∑–º–µ–Ω–µ–Ω–∏–π –ù–ï –ø—Ä–∏–º–µ–Ω–µ–Ω–æ")
        print("   –ò—Å–ø–æ–ª—å–∑—É–π --apply –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è")
        return

    # –ó–∞–≥—Ä—É–∂–∞–µ–º core –∏ –æ–±–Ω–æ–≤–ª—è–µ–º
    with open(CORE_FILE, encoding="utf-8") as f:
        core_data = json.load(f)

    # –°–æ–∑–¥–∞—ë–º backup
    backup_file = CORE_FILE.with_suffix(f".backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(backup_file, "w", encoding="utf-8") as f:
        json.dump(core_data, f, ensure_ascii=False, indent=2)
    print(f"\nüíæ Backup —Å–æ–∑–¥–∞–Ω: {backup_file.name}")

    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç—ã
    core_by_id = {q["id"]: q for q in core_data["questions"]}
    updated = 0

    for m in updates:
        if m["core_id"] in core_by_id:
            core_by_id[m["core_id"]]["text"] = m["v2_text"]
            updated += 1

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    with open(CORE_FILE, "w", encoding="utf-8") as f:
        json.dump(core_data, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ core: {updated}")


def main():
    parser = argparse.ArgumentParser(description="–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –±–∞–∑ –≤–æ–ø—Ä–æ—Å–æ–≤")
    parser.add_argument("--dry-run", action="store_true", help="–¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å –ø–ª–∞–Ω")
    parser.add_argument("--apply", action="store_true", help="–ü—Ä–∏–º–µ–Ω–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è")
    parser.add_argument("--threshold", type=float, default=0.85, help="–ü–æ—Ä–æ–≥ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ (0-1)")
    args = parser.parse_args()

    print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ v2 (–∫–Ω–∏–≥–∏)...")
    v2_questions = load_v2_questions()
    print(f"   –í–æ–ø—Ä–æ—Å–æ–≤: {len(v2_questions)}")

    print("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ core (–æ–Ω–±–æ—Ä–¥–∏–Ω–≥)...")
    core_questions = load_core_questions()
    print(f"   –í–æ–ø—Ä–æ—Å–æ–≤: {len(core_questions)}")

    print(f"\nüîç –ü–æ–∏—Å–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (threshold={args.threshold})...")
    matches = find_matches(v2_questions, core_questions, args.threshold)
    print(f"   –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(matches)}")

    updates = show_sync_plan(matches)

    if args.apply:
        apply_sync(matches, dry_run=False)
    else:
        apply_sync(matches, dry_run=True)


if __name__ == "__main__":
    main()
