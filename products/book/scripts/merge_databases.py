#!/usr/bin/env python3
"""
AI Question Merger â€” Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ· Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ñ‡ĞµÑ€ĞµĞ· Claude.

Ğ‘ĞµÑ€Ñ‘Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Ğ¸Ğ· v2 (Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ñ‹/ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ñ‹) Ğ¸ Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ Ğ² core
Ğ´Ğ»Ñ Ğ¾Ğ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸.

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:
    python merge_databases.py --analyze     # ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ±ĞµĞ· Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹
    python merge_databases.py --merge       # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½ÑƒÑ Ğ±Ğ°Ğ·Ñƒ
    python merge_databases.py --sample 10   # Ğ¢ĞµÑÑ‚ Ğ½Ğ° 10 Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ñ…
"""

import json
import asyncio
import argparse
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import Optional, List, Dict
import os
import sys

PROJECT_ROOT = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from dotenv import load_dotenv
load_dotenv(PROJECT_ROOT / ".env")

import anthropic

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞšĞĞĞ¤Ğ˜Ğ“Ğ£Ğ ĞĞ¦Ğ˜Ğ¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DATA_DIR = PROJECT_ROOT / "intelligent_question_core/data"
V2_FILE = DATA_DIR / "selfology_programs_v2.json"
CORE_FILE = DATA_DIR / "selfology_intelligent_core.json"
OUTPUT_FILE = DATA_DIR / "selfology_unified.json"

MODEL = "claude-sonnet-4-20250514"  # Claude Sonnet 4

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞŸĞ ĞĞœĞŸĞ¢Ğ«
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MATCHER_SYSTEM = """Ğ¢Ñ‹ Ğ¾Ğ¿Ñ‹Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿ÑĞ¸Ñ…Ğ¾Ñ‚ĞµÑ€Ğ°Ğ¿ĞµĞ²Ñ‚ Ñ 20-Ğ»ĞµÑ‚Ğ½Ğ¸Ğ¼ ÑÑ‚Ğ°Ğ¶ĞµĞ¼ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ Ñ€ĞµÑ„Ğ»ĞµĞºÑĞ¸Ğ²Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸.

Ğ¢Ğ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° â€” Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ¡Ğ•ĞœĞĞĞ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ• ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ Ğ¸Ğ· Ğ´Ğ²ÑƒÑ… Ğ±Ğ°Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….

Ğ’ĞĞ–ĞĞ: Ğ’Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑÑ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ñ‹ Ğ¿Ğ¾-Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¼Ñƒ, Ğ½Ğ¾ Ğ¸Ğ¼ĞµÑ‚ÑŒ ĞĞ”Ğ˜ĞĞĞšĞĞ’Ğ«Ğ™ Ğ¡ĞœĞ«Ğ¡Ğ›.

ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹:
- "Ğ§ĞµĞ¼ Ğ²Ñ‹ Ğ³Ğ¾Ñ€Ğ´Ğ¸Ñ‚ĞµÑÑŒ?" â‰ˆ "Ğ§ĞµĞ¼ Ñ‚Ñ‹ Ğ³Ğ¾Ñ€Ğ´Ğ¸ÑˆÑŒÑÑ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²ÑĞµĞ³Ğ¾?"
- "ĞĞ¿Ğ¸ÑˆĞ¸ ÑĞ²Ğ¾Ñ‘ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ" â‰ˆ "ĞšĞ°Ğº Ñ‚Ñ‹ ÑĞµĞ±Ñ Ñ‡ÑƒĞ²ÑÑ‚Ğ²ÑƒĞµÑˆÑŒ ÑĞµĞ¹Ñ‡Ğ°Ñ?"
- "Ğ Ñ‡Ñ‘Ğ¼ Ğ¼ĞµÑ‡Ñ‚Ğ°ĞµÑˆÑŒ?" â‰ˆ "ĞšĞ°ĞºĞ¸Ğµ Ñƒ Ñ‚ĞµĞ±Ñ Ğ¼ĞµÑ‡Ñ‚Ñ‹ Ğ¾ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼?"

ĞĞ• ÑĞ²Ğ»ÑÑÑ‚ÑÑ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸ÑĞ¼Ğ¸:
- "Ğ§ĞµĞ¼ Ğ³Ğ¾Ñ€Ğ´Ğ¸ÑˆÑŒÑÑ?" vs "Ğ§ĞµĞ³Ğ¾ ÑÑ‚Ñ‹Ğ´Ğ¸ÑˆÑŒÑÑ?" (Ğ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğ¹ ÑĞ¼Ñ‹ÑĞ»)
- "ĞšĞ°Ğº Ğ´ĞµĞ»Ğ° Ğ½Ğ° Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ?" vs "ĞšĞ°Ğº Ğ´ĞµĞ»Ğ° Ğ² Ğ¾Ñ‚Ğ½Ğ¾ÑˆĞµĞ½Ğ¸ÑÑ…?" (Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ Ğ´Ğ¾Ğ¼ĞµĞ½Ñ‹)

ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ¡Ğ¢Ğ ĞĞ“Ğ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ JSON."""

MATCHER_USER = """ĞĞ°Ğ¹Ğ´Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸Ğ· Ğ±Ğ°Ğ·Ñ‹ ĞšĞĞ˜Ğ“Ğ˜ Ğ² Ğ±Ğ°Ğ·Ğµ Ğ‘ĞĞ¢Ğ.

Ğ’ĞĞŸĞ ĞĞ¡ Ğ˜Ğ— ĞšĞĞ˜Ğ“Ğ˜ (v2):
ID: {v2_id}
Ğ¢ĞµĞºÑÑ‚: "{v2_text}"
ĞŸÑ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°: {program}
ĞšĞ»Ğ°ÑÑ‚ĞµÑ€: {cluster}

ĞšĞĞĞ”Ğ˜Ğ”ĞĞ¢Ğ« Ğ˜Ğ— Ğ‘ĞĞ—Ğ« Ğ‘ĞĞ¢Ğ (core):
{candidates}

Ğ’ĞµÑ€Ğ½Ğ¸ JSON:
{{
  "match_found": true/false,
  "core_id": "id Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸Ğ· core Ğ¸Ğ»Ğ¸ null",
  "confidence": 0.0-1.0,
  "reasoning": "Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ ÑÑ‚Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ (Ğ¸Ğ»Ğ¸ Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ Ğ½ĞµÑ‚)"
}}

Ğ•ÑĞ»Ğ¸ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ < 0.7, ÑÑ‚Ğ°Ğ²ÑŒ match_found: false."""

QUALITY_SYSTEM = """Ğ¢Ñ‹ Ğ¿ÑĞ¸Ñ…Ğ¾Ñ‚ĞµÑ€Ğ°Ğ¿ĞµĞ²Ñ‚-Ñ€ĞµĞ´Ğ°ĞºÑ‚Ğ¾Ñ€, Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑÑ‰Ğ¸Ğ¹ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ´Ğ»Ñ ĞºĞ½Ğ¸Ğ³Ğ¸ ÑĞ°Ğ¼Ğ¾Ğ¿Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸ AI-ĞºĞ¾ÑƒÑ‡Ğ°.

Ğ¢Ğ²Ğ¾Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° â€” Ğ²Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ›Ğ£Ğ§Ğ¨Ğ£Ğ® Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ÑŒ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ Ğ²ĞµÑ€ÑĞ¸Ñ.

ĞšĞ Ğ˜Ğ¢Ğ•Ğ Ğ˜Ğ˜:
1. Ğ‘ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ â€” Ğ½Ğµ Ñ‚Ñ€Ğ°Ğ²Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚, Ğ½Ğµ Ğ½Ğ°Ğ²ĞµÑˆĞ¸Ğ²Ğ°ĞµÑ‚ ÑÑ€Ğ»Ñ‹ĞºĞ¸
2. ĞŸÑ€Ğ¸Ğ³Ğ»Ğ°ÑˆĞµĞ½Ğ¸Ğµ â€” Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ñ€Ğ¸Ğ³Ğ»Ğ°ÑˆĞ°ĞµÑ‚ Ğº Ñ€Ğ°Ğ·Ğ¼Ñ‹ÑˆĞ»ĞµĞ½Ğ¸Ñ, Ğ½Ğµ Ğ´Ğ¾Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµÑ‚
3. Ğ¯ÑĞ½Ğ¾ÑÑ‚ÑŒ â€” Ğ¿Ğ¾Ğ½ÑÑ‚ĞµĞ½ Ğ±ĞµĞ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°
4. Ğ“Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° â€” Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾, Ğ½Ğ¾ Ğ½Ğµ Ğ¿ÑƒĞ³Ğ°ÑÑ‰Ğµ
5. Ğ¢Ğ¾Ğ½ â€” Ñ‚Ñ‘Ğ¿Ğ»Ñ‹Ğ¹, ÑƒĞ²Ğ°Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹, Ğ½Ğ° "Ñ‚Ñ‹"
6. Ğ“Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸ĞºĞ° â€” Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº, Ğ±ÑƒĞºĞ²Ğ° "Ñ‘" Ğ³Ğ´Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾

ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ¡Ğ¢Ğ ĞĞ“Ğ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ JSON."""

QUALITY_USER = """Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸ Ğ´Ğ²Ğµ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ° Ğ¸ Ğ²Ñ‹Ğ±ĞµÑ€Ğ¸ Ğ»ÑƒÑ‡ÑˆÑƒÑ (Ğ¸Ğ»Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ½ÑƒÑ).

Ğ’ĞĞ Ğ˜ĞĞĞ¢ 1 (Ğ¸Ğ· ĞºĞ½Ğ¸Ğ³Ğ¸ v2):
"{v2_text}"

Ğ’ĞĞ Ğ˜ĞĞĞ¢ 2 (Ğ¸Ğ· Ğ±Ğ¾Ñ‚Ğ° core):
"{core_text}"

ĞšĞĞĞ¢Ğ•ĞšĞ¡Ğ¢:
- ĞŸÑ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°: {program}
- ĞšĞ»Ğ°ÑÑ‚ĞµÑ€: {cluster}

Ğ’ĞµÑ€Ğ½Ğ¸ JSON:
{{
  "winner": 1 Ğ¸Ğ»Ğ¸ 2 Ğ¸Ğ»Ğ¸ 0 (ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ° Ğ½Ğ¾Ğ²Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ),
  "best_text": "Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°",
  "changes_made": ["Ñ‡Ñ‚Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¾"],
  "reasoning": "Ğ¿Ğ¾Ñ‡ĞµĞ¼Ñƒ ÑÑ‚Ğ¾Ñ‚ Ğ²Ñ‹Ğ±Ğ¾Ñ€"
}}"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ĞĞ¡ĞĞĞ’ĞĞĞ™ ĞšĞ›ĞĞ¡Ğ¡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class QuestionMerger:
    def __init__(self):
        self.client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        self.v2_data = None
        self.core_data = None
        self.core_questions = {}
        self.matches = []
        self.stats = {
            "total_v2": 0,
            "matched": 0,
            "unmatched": 0,
            "improved": 0
        }

    def load_data(self):
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¾Ğ±ĞµĞ¸Ñ… Ğ±Ğ°Ğ·"""
        print("ğŸ“š Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° v2 (ĞºĞ½Ğ¸Ğ³Ğ¸)...")
        with open(V2_FILE, encoding="utf-8") as f:
            self.v2_data = json.load(f)

        # Ğ¡Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ² v2
        for prog in self.v2_data["programs"]:
            for block in prog["blocks"]:
                self.stats["total_v2"] += len(block["questions"])

        print(f"   Ğ’Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²: {self.stats['total_v2']}")

        print("ğŸ¤– Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° core (Ğ±Ğ¾Ñ‚)...")
        with open(CORE_FILE, encoding="utf-8") as f:
            self.core_data = json.load(f)

        # Ğ˜Ğ½Ğ´ĞµĞºÑĞ¸Ñ€ÑƒĞµĞ¼ core Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹
        for q in self.core_data["questions"]:
            self.core_questions[q["id"]] = q

        print(f"   Ğ’Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²: {len(self.core_questions)}")

    async def call_claude(self, system: str, user: str) -> dict:
        """Ğ’Ñ‹Ğ·Ğ¾Ğ² Claude API"""
        try:
            message = self.client.messages.create(
                model=MODEL,
                max_tokens=1024,
                system=system,
                messages=[{"role": "user", "content": user}]
            )

            # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ JSON Ğ¸Ğ· Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
            text = message.content[0].text
            # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ñ‹Ğµ markdown Ğ±Ğ»Ğ¾ĞºĞ¸
            if "```json" in text:
                text = text.split("```json")[1].split("```")[0]
            elif "```" in text:
                text = text.split("```")[1].split("```")[0]

            return json.loads(text.strip())
        except Exception as e:
            print(f"âŒ Claude Error: {e}")
            return {"error": str(e)}

    def prepare_candidates(self, v2_text: str, limit: int = 15) -> str:
        """ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° ÑĞ¿Ğ¸ÑĞºĞ° ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ¸Ğ· core Ğ´Ğ»Ñ ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ"""
        # ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ° â€” Ğ±ĞµÑ€Ñ‘Ğ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¼Ğ¸ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼Ğ¸
        v2_words = set(v2_text.lower().split())

        scored = []
        for core_id, core_q in self.core_questions.items():
            core_words = set(core_q["text"].lower().split())
            overlap = len(v2_words & core_words)
            if overlap > 0:
                scored.append((overlap, core_id, core_q["text"]))

        # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ overlap Ğ¸ Ğ±ĞµÑ€Ñ‘Ğ¼ Ñ‚Ğ¾Ğ¿
        scored.sort(reverse=True)
        top = scored[:limit]

        # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°
        lines = []
        for i, (_, core_id, text) in enumerate(top, 1):
            lines.append(f"{i}. ID: {core_id}\n   Ğ¢ĞµĞºÑÑ‚: \"{text}\"")

        return "\n\n".join(lines) if lines else "ĞšĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ¾Ğ² Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾"

    async def find_match(self, v2_q: dict, program: str, cluster: str) -> dict:
        """ĞĞ°Ğ¹Ñ‚Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°"""
        candidates = self.prepare_candidates(v2_q["text"])

        user_prompt = MATCHER_USER.format(
            v2_id=v2_q["id"],
            v2_text=v2_q["text"],
            program=program,
            cluster=cluster,
            candidates=candidates
        )

        result = await self.call_claude(MATCHER_SYSTEM, user_prompt)

        if "error" in result:
            return {"match_found": False, "error": result["error"]}

        return result

    async def improve_question(self, v2_text: str, core_text: str,
                                program: str, cluster: str) -> dict:
        """Ğ’Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ"""
        user_prompt = QUALITY_USER.format(
            v2_text=v2_text,
            core_text=core_text,
            program=program,
            cluster=cluster
        )

        result = await self.call_claude(QUALITY_SYSTEM, user_prompt)

        if "error" in result:
            return {"winner": 1, "best_text": v2_text, "error": result["error"]}

        return result

    async def process_question(self, v2_q: dict, program: str, cluster: str) -> dict:
        """ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°"""
        # 1. ĞĞ°Ğ¹Ñ‚Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ
        match = await self.find_match(v2_q, program, cluster)

        result = {
            "v2_id": v2_q["id"],
            "v2_text": v2_q["text"],
            "program": program,
            "cluster": cluster,
            "match_found": match.get("match_found", False),
            "core_id": match.get("core_id"),
            "confidence": match.get("confidence", 0),
            "final_text": v2_q["text"],
            "core_metadata": None
        }

        # 2. Ğ•ÑĞ»Ğ¸ Ğ½Ğ°ÑˆĞ»Ğ¸ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğµ â€” Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ
        if result["match_found"] and result["core_id"]:
            core_q = self.core_questions.get(result["core_id"])
            if core_q:
                # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· core
                result["core_metadata"] = {
                    "classification": core_q.get("classification"),
                    "psychology": core_q.get("psychology"),
                    "processing_hints": core_q.get("processing_hints")
                }

                # Ğ’Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ»ÑƒÑ‡ÑˆÑƒÑ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ
                improvement = await self.improve_question(
                    v2_q["text"], core_q["text"], program, cluster
                )
                result["final_text"] = improvement.get("best_text", v2_q["text"])
                result["changes"] = improvement.get("changes_made", [])

                if result["final_text"] != v2_q["text"]:
                    self.stats["improved"] += 1

            self.stats["matched"] += 1
        else:
            self.stats["unmatched"] += 1

        return result

    async def run_analysis(self, sample_size: Optional[int] = None):
        """ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹"""
        print("\n" + "â•" * 60)
        print("ğŸ” ĞĞĞĞ›Ğ˜Ğ— Ğ¡ĞĞĞ¢Ğ’Ğ•Ğ¢Ğ¡Ğ¢Ğ’Ğ˜Ğ™ (Claude)")
        print("â•" * 60)

        results = []
        count = 0

        for prog in self.v2_data["programs"]:
            for block in prog["blocks"]:
                for q in block["questions"]:
                    if sample_size and count >= sample_size:
                        break

                    print(f"\r   ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°: {count + 1}/{sample_size or self.stats['total_v2']}", end="")

                    result = await self.process_question(q, prog["name"], block["name"])
                    results.append(result)
                    self.matches.append(result)
                    count += 1

                    # Rate limiting
                    await asyncio.sleep(0.5)

                if sample_size and count >= sample_size:
                    break
            if sample_size and count >= sample_size:
                break

        print("\n")
        self.print_stats()
        return results

    def print_stats(self):
        """Ğ’Ñ‹Ğ²Ğ¾Ğ´ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸"""
        print("\n" + "â”€" * 40)
        print("ğŸ“Š Ğ¡Ğ¢ĞĞ¢Ğ˜Ğ¡Ğ¢Ğ˜ĞšĞ")
        print("â”€" * 40)
        print(f"   ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾: {self.stats['matched'] + self.stats['unmatched']}")
        print(f"   âœ… ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹: {self.stats['matched']}")
        print(f"   âŒ Ğ‘ĞµĞ· ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ: {self.stats['unmatched']}")
        print(f"   âœï¸  Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¾ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ğº: {self.stats['improved']}")

        if self.matches:
            match_rate = self.stats['matched'] / len(self.matches) * 100
            print(f"   ğŸ“ˆ Match rate: {match_rate:.1f}%")

    def show_matches(self, limit: int = 20):
        """ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ñ"""
        print("\n" + "â•" * 60)
        print("ğŸ”— ĞĞĞ™Ğ”Ğ•ĞĞĞ«Ğ• Ğ¡ĞĞĞ¢Ğ’Ğ•Ğ¢Ğ¡Ğ¢Ğ’Ğ˜Ğ¯")
        print("â•" * 60)

        matched = [m for m in self.matches if m["match_found"]]

        for i, m in enumerate(matched[:limit], 1):
            print(f"\n{i}. [{m['program']} â†’ {m['cluster']}]")
            print(f"   Confidence: {m['confidence']:.0%}")
            print(f"   V2: {m['v2_text'][:60]}...")
            if m["final_text"] != m["v2_text"]:
                print(f"   â†’ Final: {m['final_text'][:60]}...")
            if m.get("changes"):
                print(f"   Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ: {', '.join(m['changes'])}")

        if len(matched) > limit:
            print(f"\n   ... Ğ¸ ĞµÑ‰Ñ‘ {len(matched) - limit}")

    def save_results(self, output_file: Optional[Path] = None):
        """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²"""
        output = output_file or OUTPUT_FILE

        results = {
            "generated_at": datetime.now().isoformat(),
            "stats": self.stats,
            "matches": self.matches
        }

        with open(output, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹: {output}")


async def main():
    parser = argparse.ArgumentParser(description="AI Question Merger (Claude)")
    parser.add_argument("--analyze", action="store_true", help="ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¸Ğ¹")
    parser.add_argument("--sample", type=int, help="ĞĞ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ (Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ°)")
    args = parser.parse_args()

    merger = QuestionMerger()
    merger.load_data()

    sample = args.sample or 10  # ĞŸĞ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ 10 Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ°

    print(f"\nâš ï¸  Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼: Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° {sample} Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²")
    print("   Ğ”Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ --sample N Ñ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğ¼ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼")

    await merger.run_analysis(sample_size=sample)
    merger.show_matches()
    merger.save_results(DATA_DIR / f"merge_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")


if __name__ == "__main__":
    asyncio.run(main())
